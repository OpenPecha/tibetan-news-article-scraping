{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tt_article_content(url, tags=\"གསར་འགྱུར།\"):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"No Audio in TibetTimes\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': [tags]}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.find('div', class_=\"entry-header\")\n",
    "        if title:\n",
    "            title_h1 = title.find(\"h1\", class_=\"jeg_post_title\")\n",
    "            title_text = title_h1.get_text(strip=True) if title_h1 else \"Title not found\"\n",
    "        else:\n",
    "            title_text = \"Title not found\"\n",
    "        final_response['data'][\"title\"] = title_text\n",
    "\n",
    "        \n",
    "        # Extracting Meta Data\n",
    "        try:\n",
    "            meta_data_body = soup.find('div', class_=\"jeg_post_meta jeg_post_meta_1\")\n",
    "            if meta_data_body:\n",
    "                author_name = meta_data_body.find('div', class_=\"jeg_meta_author\")\n",
    "                final_response['data']['meta_data'][\"Author\"] = author_name.get_text(strip=True) if author_name else \"Author not found\"\n",
    "                \n",
    "                date_time = meta_data_body.find('div', class_=\"jeg_meta_date\")\n",
    "                final_response['data']['meta_data'][\"Date\"] = date_time.get_text(strip=True) if date_time else \"Date not found\"\n",
    "        except AttributeError:\n",
    "            final_response['data']['meta_data'][\"Author\"] = \"Error fetching author\"\n",
    "            final_response['data']['meta_data'][\"Date\"] = \"Error fetching date\"\n",
    "\n",
    "\n",
    "        # Extract body content\n",
    "        try:\n",
    "            body = soup.find('div', class_='entry-content with-share')\n",
    "            if body:\n",
    "                # print(body)\n",
    "                inner_content = body.find(\"div\", class_=\"content-inner\")\n",
    "                if inner_content:\n",
    "                    # Extracting all <p> tags for text content\n",
    "                    paragraphs = inner_content.find_all('p')\n",
    "                    final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "                else:\n",
    "                    final_response['data']['body'][\"Text\"] = [\"No Content in the article\"]\n",
    "\n",
    "        except AttributeError as e:\n",
    "            final_response['data']['body'][\"Text\"] = [f\"Error fetching body content{str(e)}\"]\n",
    "        \n",
    "        return final_response\n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_tt_page_article_links(url: str):\n",
    "    \"\"\"\n",
    "    Extracts all article links from a given VOT (Voice of Tibet) webpage.\n",
    "\n",
    "    This function scrapes the provided URL and extracts links to individual articles\n",
    "    found on the page.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the VOT webpage containing article links.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"Links\": List[],\n",
    "            \"Message\": string,\n",
    "            \"Response\": int\n",
    "        }\n",
    "    Raises:\n",
    "    requests.RequestException: If there's an error fetching the webpage.\n",
    "    ValueError: If the expected HTML structure is not found on the page.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    final_response = {\n",
    "        \"Links\": [],\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=(5, 60-5))\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "\n",
    "        if end_time-start_time > 50:\n",
    "            print(f\"This ULR Took more then 50s: {url}\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extracting all the articles in the DIV\n",
    "        all_article = soup.find_all(\"h3\", class_=\"jeg_post_title\")\n",
    "        print(type(all_article))\n",
    "        if not all_article:\n",
    "            raise ValueError(\"Could not find the main article container on the page.\")\n",
    "        \n",
    "        \n",
    "        # # Getting all the links of articles \n",
    "        all_links = []\n",
    "        for each_head in all_article:\n",
    "            article_links = each_head.find(\"a\")\n",
    "            if article_links is not None:\n",
    "                all_links.append(article_links.get(\"href\"))\n",
    "        final_response[\"Links\"] = all_links\n",
    "        return final_response\n",
    "     \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"An error occurred while fetching the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except ValueError as e:\n",
    "        # print(f\"An error occurred while parsing the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while parsing the webpage: {e}\"\n",
    "        final_response[\"Response\"] = 404\n",
    "        # getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred: {e}\")\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bs4.element.ResultSet'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Links': ['https://tibettimes.net/2024/06/25/234859/',\n",
       "  'https://tibettimes.net/2024/06/24/234853/',\n",
       "  'https://tibettimes.net/2024/06/21/234741/',\n",
       "  'https://tibettimes.net/2024/06/13/234250/',\n",
       "  'https://tibettimes.net/2024/06/11/234178/',\n",
       "  'https://tibettimes.net/2024/06/04/233976/',\n",
       "  'https://tibettimes.net/2024/06/03/233910/',\n",
       "  'https://tibettimes.net/2024/06/03/233904/'],\n",
       " 'Message': 'Success',\n",
       " 'Response': 200}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "base_url = \"https://tibettimes.net/category/news/dalai-lama/page/2/\"\n",
    "num_articles = 2  # Specify the number of articles to scrape\n",
    "extract_all_tt_page_article_links(base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://tibettimes.net/2024/06/03/233904/\"\n",
    "scrape_tt_article_content(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_article_page(total_page, custom_url, key_code, print_log=15):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return_file = {\n",
    "        \"Data\": [],\n",
    "        \"message\": \"success\",\n",
    "        \"response\": 200\n",
    "    }\n",
    "    All_url_links = {}\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        for i in range(1, total_page):\n",
    "            final_url = custom_url + str(i) +\"/\"\n",
    "            found_url_links = TibetTimes_utils.extract_all_tt_page_article_links(final_url)\n",
    "            key = key_code + str(i)\n",
    "            All_url_links[key] = found_url_links\n",
    "            if i%print_log == 0:\n",
    "                end_time = time.time()\n",
    "                differnce_time = end_time-start_time\n",
    "                ratio = (total_page - i)/i\n",
    "                remaning_time = ratio * differnce_time\n",
    "                print(f\"Processed Time {differnce_time} : Remaning time {remaning_time}\")\n",
    "        return_file[\"Data\"] = All_url_links\n",
    "        return return_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        return_file[\"Data\"] = All_url_links\n",
    "        return_file[\"message\"] = e\n",
    "        return_file[\"response\"] = 404\n",
    "        return return_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
