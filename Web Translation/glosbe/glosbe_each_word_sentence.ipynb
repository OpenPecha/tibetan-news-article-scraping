{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tashi\\AppData\\Roaming\\Python\\Python39\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop_article_page(total_page, custom_url, key_code):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    return_file = {\n",
    "        \"Data\": [],\n",
    "        \"message\": \"success\",\n",
    "        \"response\": 200\n",
    "    }\n",
    "    All_url_links = {}\n",
    "    \n",
    "    try:\n",
    "        for i in tqdm(range(1, total_page)):\n",
    "            final_url = custom_url + str(i) \n",
    "            found_url_links, load_more = (final_url)\n",
    "            key = key_code + str(i)\n",
    "            All_url_links[key] = found_url_links\n",
    "            if load_more==False:\n",
    "                print(f\"Final page number: {i}\")\n",
    "                break\n",
    "        return_file[\"Data\"] = All_url_links\n",
    "        return return_file\n",
    "    \n",
    "    except Exception as e:\n",
    "        return_file[\"Data\"] = All_url_links\n",
    "        return_file[\"message\"] = e\n",
    "        return_file[\"response\"] = 404\n",
    "        return return_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.DataFrame(columns=[\"Tibetan\", \"English\", \"Author\"])\n",
    "def extract_all_sentence(url):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    global all_data\n",
    "\n",
    "    sentence_format = {\n",
    "        \"data\": {\n",
    "            \"English\":\"\",\n",
    "            \"Tibetan\":\"\",\n",
    "            \"Author\":\"\"\n",
    "        }\n",
    "    }\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    final_response = {\n",
    "        \"Links\": [],\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200,\n",
    "        \"source_url\": url\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=(5, 60-5))\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "\n",
    "        if end_time-start_time > 50:\n",
    "            print(f\"This ULR Took more then 50s: {url}\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # # Getting all the links of articles \n",
    "        sentence_list = []\n",
    "\n",
    "        # Extracting all the articles in the DIV\n",
    "        all_senetence_in_page = soup.find_all(\"div\", class_=\"odd:bg-slate-100 px-1\")\n",
    "        if all_senetence_in_page:\n",
    "            for senetnce in all_senetence_in_page:\n",
    "                tibetan_sentence = senetnce.find(\"div\", class_=\"w-1/2 dir-aware-pr-1 dense\")\n",
    "                tibetan_sentence = tibetan_sentence.get_text(strip=True) if tibetan_sentence else \"\"\n",
    "                english_sentence = senetnce.find(\"div\", class_=\"w-1/2 dir-aware-pl-1\")\n",
    "                english_sentence = english_sentence.get_text(strip=True) if english_sentence else \"\"\n",
    "                author_name = senetnce.find(\"aside\", class_=\"truncate dir-aware-text-right w-24 inline-block text-xs text-gray-500 leading-6\")\n",
    "                author_name = author_name.get_text(strip=True) if author_name else \"\"\n",
    "\n",
    "                sentence_list.append({\"Tibetan\": tibetan_sentence, \"English\": english_sentence, \"Author\": author_name})\n",
    "        \n",
    "        # Create a new DataFrame from the sentence_list\n",
    "        df = pd.DataFrame(sentence_list)\n",
    "        all_data = pd.concat([all_data, df], ignore_index=True)\n",
    "\n",
    "        final_response[\"Links\"] = sentence_list\n",
    "        nextpage = soup.find(\"div\", class_=\"text-center mt-4\")\n",
    "        if nextpage:\n",
    "            return final_response, True\n",
    "\n",
    "        return final_response, False\n",
    "     \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"An error occurred while fetching the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except ValueError as e:\n",
    "        # print(f\"An error occurred while parsing the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while parsing the webpage: {e}\"\n",
    "        final_response[\"Response\"] = 404\n",
    "        # getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred: {e}\")\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://glosbe.com/bo/en/%E0%BD%95%E0%BC%8B%E0%BD%A3%E0%BD%82%E0%BD%A6/fragment/tmem?page=1&mode=MUST&stem=true\"\n",
    "\n",
    "# extract_all_sentence(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and save Json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(path, file_name, data):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(path+file_name, \"w\") as outfile:\n",
    "        json.dump(data, outfile, indent=4, ensure_ascii=False)\n",
    "        print(f\"Successfully saved: {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_json(path, file_name):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    with open(path+file_name, 'r') as openfile:\n",
    "        # Reading from json file\n",
    "        Loaded_file = json.load(openfile)\n",
    "        print(f\"Successfully loaded: {file_name}\")\n",
    "\n",
    "    return Loaded_file\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading all Words from \"glosbe_top_words_bo_en.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded: glosbe_top_words_bo_en.json\n"
     ]
    }
   ],
   "source": [
    "path = \"./data/\"\n",
    "filename = \"glosbe_top_words_bo_en.json\"\n",
    "all_sentence_link = read_json(path, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_Words = all_sentence_link[\"Words\"]\n",
    "len(all_Words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_word_sentence = {}\n",
    "def extracted_all_page(word):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    global all_word_sentence\n",
    "    page_number = 1\n",
    "\n",
    "    while 1:\n",
    "        url = f\"https://glosbe.com/bo/en/{word}/fragment/tmem?page={page_number}&mode=MUST&stem=true\"\n",
    "        all_senetence_page, next_page = extract_all_sentence(url)\n",
    "        key = word + \" page \" + str(page_number)\n",
    "        all_word_sentence[key] = all_senetence_page\n",
    "        if next_page == False:\n",
    "            break\n",
    "        page_number += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 392/392 [19:27<00:00,  2.98s/it]\n"
     ]
    }
   ],
   "source": [
    "for word in tqdm(all_Words):\n",
    "    extracted_all_page(word)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total words added and after removing duplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total data extracted: 28640\n"
     ]
    }
   ],
   "source": [
    "print(f\"total data extracted: {all_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total clean data: 824\n"
     ]
    }
   ],
   "source": [
    "clean_sentence_df  = all_data.drop_duplicates().reset_index(drop=True)\n",
    "clean_sentence_df.shape\n",
    "print(f\"total clean data: {clean_sentence_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## saving all the files including csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved: Total_sentence_loaded.json\n"
     ]
    }
   ],
   "source": [
    "save_json(path, \"Total_sentence_loaded.json\", all_word_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved: Clean_sentence.json\n"
     ]
    }
   ],
   "source": [
    "clean_sentence_json = clean_sentence_df.to_json(force_ascii=False)\n",
    "save_json(path, \"Clean_sentence.json\", clean_sentence_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_sentence_df.to_csv(path + \"Clean_sentence.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
