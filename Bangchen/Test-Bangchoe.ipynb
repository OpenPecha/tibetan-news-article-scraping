{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any, List\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_all_Bangchen_article_links(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts all article links from a given Bangchen webpage.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the Bangchen webpage containing article links.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing article links and status details.\n",
    "    \"\"\"\n",
    "    \n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    final_response = {\n",
    "        \"Links\": [],\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200,\n",
    "        \"source_url\": url\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=(5, 60-5))\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        if end_time - start_time > 50:\n",
    "            print(f\"This URL took more than 50s: {url}\")\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_div = soup.find(\"div\", class_=\"content-area\")\n",
    "        all_articles = article_div.find_all(\"div\", class_=\"post-thumbnail\")\n",
    "        if not all_articles:\n",
    "            raise ValueError(\"Could not find the main article container on the page.\")\n",
    "        \n",
    "        article_links = []\n",
    "        for article in all_articles:\n",
    "            links = article.find(\"a\")\n",
    "            if links.get(\"href\"):\n",
    "                article_links.append(links.get(\"href\"))\n",
    "        \n",
    "        final_response[\"Links\"] = article_links\n",
    "        return final_response\n",
    "    \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "    except ValueError as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while parsing the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Links': ['https://bangchen.tibetexpress.net/%e0%bd%91%e0%bc%8b%e0%bd%96%e0%bd%a2%e0%bc%8b%e0%bd%a8%e0%bc%8b%e0%bd%a2%e0%bd%b2%e0%bd%a0%e0%bd%b2%e0%bc%8b%e0%bd%a6%e0%be%b2%e0%bd%b2%e0%bd%91%e0%bc%8b%e0%bd%a0%e0%bd%9b%e0%bd%b2%e0%bd%93%e0%bc%8b-2/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%a8%e0%bc%8b%e0%bd%a2%e0%bd%b2%e0%bd%a0%e0%bd%b2%e0%bc%8b%e0%bd%91%e0%bd%98%e0%bd%82%e0%bc%8b%e0%bd%98%e0%bd%b2%e0%bc%8b%e0%bd%9e%e0%bd%b2%e0%bd%82%e0%bc%8b%e0%bd%82%e0%bd%b2%e0%bd%a6%e0%bc%8b/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%a2%e0%be%92%e0%be%b1%e0%bc%8b%e0%bd%82%e0%bd%a2%e0%bc%8b%e0%bd%82%e0%be%b1%e0%bd%b2%e0%bc%8b%e0%bd%82%e0%bd%9e%e0%bd%b4%e0%bd%84%e0%bc%8b%e0%bd%9a%e0%bd%96%e0%bc%8b%e0%bd%9f%e0%bd%b4%e0%bd%a2/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%a6%e0%bd%b4%e0%bd%91%e0%bc%8b%e0%bd%a6%e0%bd%b2%e0%bc%8b%e0%bd%91%e0%bd%84%e0%bc%8b%e0%bd%a3%e0%bd%ba%e0%bd%82%e0%bc%8b%e0%bd%8f%e0%bd%ba%e0%bd%93%e0%bc%8b%e0%bd%a6%e0%bd%b2%e0%bd%8a%e0%bc%8b/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%80%e0%bd%bc%e0%bd%a3%e0%bc%8b%e0%bd%80%e0%bc%8b%e0%bd%8f%e0%bd%a2%e0%bc%8b%e0%bd%a6%e0%be%a8%e0%bd%93%e0%bc%8b%e0%bd%94%e0%bc%8b%e0%bd%96%e0%bd%b4%e0%bc%8b%e0%bd%98%e0%bd%bc%e0%bc%8b%e0%bd%9e/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%a3%e0%bd%b4%e0%bd%82%e0%bc%8b%e0%bd%a3%e0%be%a4%e0%bd%82%e0%bd%a6%e0%bc%8b%e0%bd%82%e0%be%b1%e0%bd%bc%e0%bd%93%e0%bc%8b%e0%bd%94%e0%bd%a0%e0%bd%b2%e0%bc%8b%e0%bd%a6%e0%be%a4%e0%be%b1%e0%bd%84/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%96%e0%be%b7%e0%bd%84%e0%bc%8b%e0%bd%a3%e0%bc%8b%e0%bd%91%e0%be%b7%e0%bd%ba%e0%bc%8b%e0%bd%a4%e0%bd%b2%e0%bd%a0%e0%bd%b2%e0%bc%8b%e0%bd%a6%e0%be%b2%e0%bd%b2%e0%bd%91%e0%bc%8b%e0%bd%96%e0%be%b3/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%a6%e0%bc%8b%e0%bd%a2%e0%bd%b1%e0%bc%8b%e0%bd%98%e0%bd%90%e0%bd%bc%e0%bc%8b%e0%bd%a6%e0%be%b3%e0%bd%bc%e0%bd%96%e0%bc%8b%e0%bd%91%e0%bd%96%e0%bd%b4%e0%bc%8b%e0%bd%96%e0%bd%a2%e0%be%99%e0%bd%ba/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%a8%e0%bd%bc%e0%bc%8b%e0%bd%82%e0%be%b3%e0%bd%b2%e0%bd%84%e0%bc%8b%e0%bd%86%e0%bd%b4%e0%bc%8b%e0%bd%96%e0%bd%9e%e0%bd%b2%e0%bc%8b%e0%bd%a6%e0%be%92%e0%bd%84%e0%bc%8b%e0%bd%91%e0%be%b2%e0%bd%b4/',\n",
       "  'https://bangchen.tibetexpress.net/%e0%bd%91%e0%bc%8b%e0%bd%96%e0%bd%a2%e0%bc%8b%e0%bd%a0%e0%bd%9b%e0%bd%98%e0%bc%8b%e0%bd%82%e0%be%b3%e0%bd%b2%e0%bd%84%e0%bc%8b%e0%bd%82%e0%bd%b2%e0%bc%8b%e0%bd%a1%e0%bd%84%e0%bc%8b%e0%bd%a2%e0%be%a9/'],\n",
       " 'Message': 'Success',\n",
       " 'Response': 200,\n",
       " 'source_url': 'https://bangchen.tibetexpress.net/page/2/'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://bangchen.tibetexpress.net/page/2/\"\n",
    "\n",
    "extract_all_Bangchen_article_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_Bangchen_article_content(url):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"No Audio in Bangchen\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': []}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        Tag_title_date = soup.find(\"div\", class_=\"entry-header-details\")\n",
    "        \n",
    "        # Extract title\n",
    "        title_h1 = Tag_title_date.find(\"h1\", class_=\"entry-title\")\n",
    "        if title_h1:\n",
    "            title_text = title_h1.get_text(strip=True) if title_h1 else \"Title not found\"\n",
    "        else:\n",
    "            title_text = \"Title not found\"\n",
    "        final_response['data'][\"title\"] = title_text\n",
    "\n",
    "        # for tags\n",
    "        tag_list = []\n",
    "        All_Tags = Tag_title_date.find_all('a', class_=\"covernews-categories category-color-1\")\n",
    "        for each_tag in All_Tags:\n",
    "            if each_tag.get_text():\n",
    "                tag_list.append(each_tag.get_text(strip=True))\n",
    "        final_response['data']['meta_data'][\"Tags\"] = tag_list\n",
    "        \n",
    "        # Extracting Meta Data\n",
    "        try:\n",
    "            meta_data_body = Tag_title_date.find('span', class_=\"author-links\")\n",
    "            if meta_data_body:\n",
    "                author_name = meta_data_body.find('span', class_=\"item-metadata posts-author\")\n",
    "                final_response['data']['meta_data'][\"Author\"] = author_name.get_text(strip=True) if author_name else \"Author not found\"\n",
    "                \n",
    "                date_time = meta_data_body.find('span', class_=\"item-metadata posts-date\")\n",
    "                final_response['data']['meta_data'][\"Date\"] = date_time.get_text(strip=True) if date_time else \"Date not found\"\n",
    "        except AttributeError:\n",
    "            final_response['data']['meta_data'][\"Author\"] = \"Error fetching author\"\n",
    "            final_response['data']['meta_data'][\"Date\"] = \"Error fetching date\"\n",
    "\n",
    "\n",
    "        # Extract body content\n",
    "        try:\n",
    "            body = soup.find('div', class_='entry-content')\n",
    "            if body:\n",
    "                # print(body)\n",
    "                # Extracting all <p> tags for text content\n",
    "                paragraphs = body.find_all('p')\n",
    "                final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "            \n",
    "            else:\n",
    "                final_response['data']['body'][\"Text\"] = [\"No Content in the article\"]\n",
    "\n",
    "        except AttributeError as e:\n",
    "            final_response['data']['body'][\"Text\"] = [f\"Error fetching body content{str(e)}\"]\n",
    "        \n",
    "        return final_response\n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'title': 'ཨ་རིའི་དམག་མི་ཞིག་གིས་རྒྱལ་ཁབ་ཀྱི་གསང་བའི་གནས་ཚུལ་མང་པོ་རྒྱ་ནག་ལ་མཁོ་སྤྲོད་བྱས་པ་ངོས་ལེན་བྱས་ཡོད་འདུག',\n",
       "  'body': {'Audio': 'No Audio in Bangchen',\n",
       "   'Text': ['ཟླ་འདིའི་ཚེས་༡༤ ་ཉིན་རྒྱལ་སྤྱིའི་གསར་འགྱུར་བརྒྱུད་ལམ་ཁག་ཏུ་གནས་ཚུལ་སྤེལ་གསལ་ལྟར་ན། ཨ་རིའི་དམག་མི་ཞིག་གིས་རྒྱ་ནག་ལ་རྒྱལ་ཁབ་སྲུང་སྐྱོབ་དང་འབྲེལ་བའི་གསང་བའི་གནས་ཚུལ་མཁོ་སྤྲོད་བྱས་ཡོད་པའི་ནག་ཉེས་ངོས་ལེན་བྱས་འདུག',\n",
       "    'ཨ་རིའི་དམག་མི་ཤུལ་ཙི་( Korbein Schultz) ་ཟེར་བ་ཞིག་གིས་ལོ་འདིའི་ཕྱི་ཟླ་༣ ་པའི་ནང་འཛིན་བཟུང་བྱས་ཡོད་འདུག་པ་དང་། ཁོས་ཧོང་ཀོང་དུ་ཡོད་པའི་མི་ཞིག་གི་ལག་ནས་ཨ་སྒོར་ཁྲི་༤།༢ ་བླངས་ནས་ཨ་རིའི་དམག་མིའི་གསང་བའི་ཡིག་ཆ་བཅུ་ཕྲག་མང་པོ་མི་དེ་ལ་མཁོ་སྤྲོད་བྱས་ཡོད་འདུག ཤུལ་ཙི་ཡིས་ཧོང་ཀོང་དུ་ཡོད་པའི་མི་དེ་ནི་རྒྱ་གཞུང་དང་འབྲེལ་བ་ཡོད་པར་ཡིད་ཆེས་བྱེད་ཀྱི་ཡོད་འདུག',\n",
       "    'ཁོས་མཁོ་སྤྲོད་བྱས་པའི་ཡིག་ཆའི་ཁྲོད། ཨ་རིའི་དམག་གི་ཐབས་བྱུས་དང་གོ་མཚོན་གྱི་གནས་ཚུལ། ལྷག་པར་དུ་མ་འོངས་པར་ཐའེ་ཝན་སྲུང་སྐྱོབ་ལ་བེད་སྤྱོད་བྱེད་ཆོག་པའི་ད་ཐེངས་ཨ་རིའི་དམག་མི་ཚོ་ལ་ཡུག་རེན་དང་ཨུ་རུ་སུའི་དམག་འཁྲུག་ལས་ཐོབ་པའི་བསླབ་བྱ། \\xa0གཞན་ཡང་ཨ་རིའི་དམག་སྦྱོང་དང་། HH-60 ཞེས་པའི་ཐད་འཕུར་གནམ་གྲུ་དང་། F-22A ཞེས་པའི་འཐབ་འཛིང་གནམ་གྲུ U-2ཞེས་པའི་གསང་ལྟའི་གནམ་གྲུ་སོགས་ཀྱི་གནས་ཚུལ་མང་དག་ཅིག་ཚུད་ཡོད་འདུག',\n",
       "    'ཨ་རིའི་ཁྲིམས་ཞིབ་ལས་ཁུངས་(Justice Department) ་ཀྱིས་བརྗོད་པ་ལྟར་ན། ཤུལ་ཙིས་ཆོག་མཆན་མེད་པར་རྒྱལ་ཁབ་སྲུང་སྐྱོབ་དང་འབྲེལ་བའི་གསང་བའི་གནས་ཚུལ་བསྡུ་རུབ་དང་ཕྱིར་བསྒྲགས་བྱས་པ་དང་། ལྐོག་རྔན་བླངས་པའི་ནག་ཉེས་ངོས་ལེན་བྱས་ཡོད་འདུག ཨ་རིའི་གཞུང་གིས་ལོ་རྗེས་མའི་ཕྱི་ཟླ་༡ ཚེས་༢༣་ཉིན་ཁོ་ལ་ཁྲིམས་ཐག་གཅོད་འཆར་ཡོད་འདུག',\n",
       "    'ཨ་རིའི་རྒྱལ་ཁབ་བདེ་འཇགས་སྡེ་ཚན་གྱི་འགན་འཛིན་ལས་རོགས་རོ་བྷར་ཊི་ཝེལ་སི་ཡིས་རྒྱ་ནག་ལྟ་བུའི་སྲིད་གཞུང་གིས་ང་ཚོའི་དམག་མི་དང་རྒྱལ་ཁབ་བདེ་འཇགས་ཀྱི་གནས་ཚུལ་བཙན་ཤེད་ཀྱིས་དམིགས་འབེན་བྱེད་བཞིན་ཡོད་པ་དང་། ང་ཚོས་ནུས་པ་ཡོད་ཚད་བཏོན་ནས་གནས་ཚུལ་དེ་ཚོ་ཕྱིའི་རྒྱལ་ཁབ་ཁག་གི་ལག་ཏུ་མི་ཤོར་བར་སྲུང་སྐྱོབ་བྱེད་རྒྱུ་ཡིན་ཞེས་བརྗོད་འདུག',\n",
       "    'དེ་སྔ་ཨ་རིའི་གཞུང་གིས་ཀེ་ལི་ཧྥོ་ནི་ཡ་(California) ་ལ་ཝེན་ཧེང་ཀྲའོ་(Wenheng Zhao) ་དང་ཅིན་ཁྲའོ་ཝེ་(Jinchao Wei) ་ཟེར་བའི་དམག་མི་གཉིས་ཀྱིས་རྒྱ་གཞུང་ལ་གསང་བ་མཁོ་སྤྲོད་བྱས་པའི་ཉེས་མིང་འོག་འཛིན་བཟུང་བྱས་པའི་རྗེས་སུ། ད་ཐེངས་དམག་མི་དེ་འཛིན་བཟུང་བྱས་ཡོད་འདུག།',\n",
       "    'གསར་འགོད་པ། བློ་བཟང་རྒྱ་མཚོ།']},\n",
       "  'meta_data': {'URL': 'https://bangchen.tibetexpress.net/%e0%bd%a8%e0%bc%8b%e0%bd%a2%e0%bd%b2%e0%bd%a0%e0%bd%b2%e0%bc%8b%e0%bd%91%e0%bd%98%e0%bd%82%e0%bc%8b%e0%bd%98%e0%bd%b2%e0%bc%8b%e0%bd%9e%e0%bd%b2%e0%bd%82%e0%bc%8b%e0%bd%82%e0%bd%b2%e0%bd%a6%e0%bc%8b/',\n",
       "   'Author': 'lobsang la',\n",
       "   'Date': 'August 15, 2024',\n",
       "   'Tags': ['Front Slider', 'News', 'རྒྱལ་སྤྱི།']}},\n",
       " 'Message': 'Success',\n",
       " 'Response': 200}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://bangchen.tibetexpress.net/%e0%bd%a8%e0%bc%8b%e0%bd%a2%e0%bd%b2%e0%bd%a0%e0%bd%b2%e0%bc%8b%e0%bd%91%e0%bd%98%e0%bd%82%e0%bc%8b%e0%bd%98%e0%bd%b2%e0%bc%8b%e0%bd%9e%e0%bd%b2%e0%bd%82%e0%bc%8b%e0%bd%82%e0%bd%b2%e0%bd%a6%e0%bc%8b/\"\n",
    "scrape_Bangchen_article_content(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
