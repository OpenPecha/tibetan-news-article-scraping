{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "def extract_all_article(url: str):\n",
    "    \"\"\"\n",
    "    Extracts all article links from a given VOT (Voice of Tibet) webpage.\n",
    "\n",
    "    This function scrapes the provided URL and extracts links to individual articles\n",
    "    found on the page.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the VOT webpage containing article links.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of URLs to individual articles.\n",
    "\n",
    "    Raises:\n",
    "    requests.RequestException: If there's an error fetching the webpage.\n",
    "    ValueError: If the expected HTML structure is not found on the page.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    final_response = {\n",
    "        \"Links\": [],\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=(5, 60-5))\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        if end_time-start_time > 50:\n",
    "            print(f\"This ULR Took more then 50s: {url}\")\n",
    "            \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        # Extracting all the articles in the DIV\n",
    "        all_article = soup.find(\"div\", class_=\"td_block_inner tdb-block-inner td-fix-index\")\n",
    "        if not all_article:\n",
    "            raise ValueError(\"Could not find the main article container on the page.\")\n",
    "        \n",
    "        # Getting all the links of articles \n",
    "        article_links = all_article.find_all(\"a\", class_=\"td-image-wrap\")\n",
    "        all_links = [link.get(\"href\") for link in article_links if link.get(\"href\")]\n",
    "        final_response[\"Links\"] = all_links\n",
    "        return final_response\n",
    "    \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"An error occurred while fetching the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except ValueError as e:\n",
    "        # print(f\"An error occurred while parsing the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while parsing the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred: {e}\")\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def scrape_vot_article(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrapes an article from the VOT (Voice of Tibet) website.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the VOT article to scrape.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing the scraped information and status details:\n",
    "        {\n",
    "            'data': {\n",
    "                'title': str,\n",
    "                'body': {\n",
    "                    'Audio': str,\n",
    "                    'Text': List[str]\n",
    "                },\n",
    "                'meta_data': {\n",
    "                    'Author': str,\n",
    "                    'Date': str,\n",
    "                    'Tags': List[str],\n",
    "                    'URL': str\n",
    "                }\n",
    "            },\n",
    "            'Message': str,\n",
    "            'Response': int\n",
    "        }\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': []}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.find('h1', class_='tdb-title-text')\n",
    "        final_response['data']['title'] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "        # Extracting Meta Data\n",
    "        try:\n",
    "            meta_data_body = soup.find('div', class_=\"vc_column_inner tdi_85 wpb_column vc_column_container tdc-inner-column td-pb-span6\")\n",
    "            if meta_data_body:\n",
    "                author_name = meta_data_body.find('a', class_=\"tdb-author-name\")\n",
    "                final_response['data']['meta_data'][\"Author\"] = author_name.get_text() if author_name else \"Author not found\"\n",
    "                \n",
    "                date_time = meta_data_body.find('time', class_=\"entry-date updated td-module-date\")\n",
    "                final_response['data']['meta_data'][\"Date\"] = date_time.get_text() if date_time else \"Date not found\"\n",
    "        except AttributeError:\n",
    "            final_response['data']['meta_data'][\"Author\"] = \"Error fetching author\"\n",
    "            final_response['data']['meta_data'][\"Date\"] = \"Error fetching date\"\n",
    "\n",
    "        # Getting tag meta data \n",
    "        try:\n",
    "            tag_meta = soup.find('ul', class_='tdb-tags')\n",
    "            if tag_meta:\n",
    "                tag_meta = tag_meta.select('li a')\n",
    "                final_response['data']['meta_data'][\"Tags\"] = [tag.text for tag in tag_meta]\n",
    "        except AttributeError:\n",
    "            final_response['data']['meta_data'][\"Tags\"] = []\n",
    "\n",
    "        # Extract body content\n",
    "        try:\n",
    "            body = soup.find('div', class_='td_block_wrap tdb_single_content tdi_100 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')\n",
    "            if body:\n",
    "                # Extracting all <p> tags for text content\n",
    "                paragraphs = body.find_all('p')\n",
    "                final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "\n",
    "                # Find the audio tag and get its src attribute\n",
    "                audio = body.find('figure', class_='wp-block-audio')\n",
    "                if audio:\n",
    "                    audio_tag = audio.find('audio')\n",
    "                    if audio_tag:\n",
    "                        final_response['data']['body'][\"Audio\"] = audio_tag.get('src', \"No audio source found\")\n",
    "        except AttributeError:\n",
    "            final_response['data']['body'][\"Text\"] = [\"Error fetching body content\"]\n",
    "        \n",
    "        return final_response\n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit ('base': conda)",
   "language": "python",
   "name": "python3912jvsc74a57bd0c5477a11df22e68b4d4a98cd37ed7b1241c33724c85053f7e5e39a3d8474085a"
  },
  "language_info": {
   "name": "python",
   "version": ""
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}