{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a7d67bb-5be9-45c7-a6b3-6223b36ee383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import Tag\n",
    "from tqdm import tqdm\n",
    "import tbwriters_utils\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119b597b-9082-413a-8efd-0c60a714d878",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "201514da-b27a-447f-92ad-4255528e7d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name_code = \"tbwriters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69ee8588-92ef-426e-83d8-f63ee7dd8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import json\n",
    "import os\n",
    "from bs4 import Tag\n",
    "from tqdm.asyncio import tqdm\n",
    "from aiolimiter import AsyncLimiter\n",
    "\n",
    "class CustomJSONEncoder(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, Tag):\n",
    "            return obj.get_text()\n",
    "        return str(obj)\n",
    "\n",
    "def save_json(path, file_name, data):\n",
    "    try: \n",
    "        with open(os.path.join(path, file_name), \"w\", encoding='utf-8') as outfile:\n",
    "            json.dump(data, outfile, indent=4, ensure_ascii=False, cls=CustomJSONEncoder)\n",
    "        print(f\"Successfully saved: {file_name}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving {file_name}: {str(e)}\")\n",
    "\n",
    "async def scrape_article(session, url, page_key_code, semaphore):\n",
    "    async with semaphore:\n",
    "        try:\n",
    "            async with session.get(url) as response:\n",
    "                content = await response.text()\n",
    "                article_content = tbwriters_utils.scrape_tbwriters_article_content(content, tags=page_key_code)\n",
    "                return article_content\n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {url}: {str(e)}\")\n",
    "            return None\n",
    "\n",
    "async def get_content(All_links_data, Total_length, page_key_code, page_key_list):\n",
    "    all_article = {}\n",
    "    save_interval = 10  # Save every 10 articles\n",
    "    save_counter = 0\n",
    "    rate_limit = AsyncLimiter(40, 1)  # 40 requests per second\n",
    "    semaphore = asyncio.Semaphore(10)  # Limit concurrent requests\n",
    "\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = []\n",
    "        for i in range(1, Total_length + 1):\n",
    "            page_key = page_key_code + str(i)\n",
    "            all_link_page = All_links_data[page_key][\"Links\"]\n",
    "            \n",
    "            for url in all_link_page:\n",
    "                task = asyncio.ensure_future(scrape_article(session, url, page_key_code, semaphore))\n",
    "                tasks.append((page_key, url, task))\n",
    "\n",
    "        for page_key, url, task in tqdm(tasks, total=len(tasks)):\n",
    "            try:\n",
    "                article_content = await task\n",
    "                if article_content:\n",
    "                    article_key = f\"{page_key}_{file_name_code}_Article_{len([k for k in all_article if k.startswith(page_key)]) + 1}\"\n",
    "                    all_article[article_key] = article_content\n",
    "                    \n",
    "                    save_counter += 1\n",
    "                    if save_counter >= save_interval:\n",
    "                        save_file_name = f\"{file_name_code}_ALL_content_{page_key_list[1]}_partial.json\"\n",
    "                        path = \"./data/parallel_content_async/\"\n",
    "                        save_json(path, save_file_name, all_article)\n",
    "                        save_counter = 0\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {url}: {str(e)}\")\n",
    "\n",
    "    Failure_count = sum(1 for article in all_article.values() if article[\"Response\"] != 200)\n",
    "    print(f\"Total Failure in the {page_key_list[1]} article: {Failure_count}\")\n",
    "    \n",
    "    # Save the final complete file\n",
    "    save_file_name = f\"{file_name_code}_ALL_content_{page_key_list[1]}.json\"\n",
    "    print(f\"Saving final file: {save_file_name}\")\n",
    "    path = \"./data/parallel_content/\"\n",
    "    save_json(path, save_file_name, all_article)\n",
    "\n",
    "# You'll need to modify your process_json_file function to use asyncio:\n",
    "async def process_json_file(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            All_links_data = json.load(file)\n",
    "            Total_length = len(All_links_data)\n",
    "            print(f\"Total page in {os.path.basename(file_path)}: {Total_length}\")\n",
    "        \n",
    "            print(f\"page key name: {list(All_links_data.keys())[-1]}\")\n",
    "            page_key_list = list(All_links_data.keys())[-1].split(\" \")\n",
    "            \n",
    "            page_key_code = \"Page \"+page_key_list[1]+\" \"\n",
    "            print(f\"Page key code: {page_key_code}\")\n",
    "            await get_content(All_links_data, Total_length, page_key_code, page_key_list)\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(f\"Error decoding JSON in file {os.path.basename(file_path)}: {str(e)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {os.path.basename(file_path)}: {str(e)}\")\n",
    "\n",
    "# And modify your get_json_files function to use asyncio:\n",
    "async def get_json_files(directory):\n",
    "    json_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.json')]\n",
    "    \n",
    "    tasks = [process_json_file(file) for file in json_files]\n",
    "    await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(f\"Processed {len(json_files)} files\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06b3dc37-3602-43b2-abee-92e5f5bf7e9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:5\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# # Example usage\n",
    "# directory_path = \n",
    "\n",
    "\n",
    "# asyncio.run(get_json_files(directory_path))\n",
    "\n",
    "\n",
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    # Replace 'directory' with your actual directory path\n",
    "    directory = './data/links/'\n",
    "    await get_json_files(directory)\n",
    "\n",
    "# Use this instead of asyncio.run()\n",
    "def run_async_code():\n",
    "    loop = asyncio.new_event_loop()\n",
    "    asyncio.set_event_loop(loop)\n",
    "    try:\n",
    "        loop.run_until_complete(main())\n",
    "    finally:\n",
    "        loop.close()\n",
    "\n",
    "# Call this function to run your async code\n",
    "run_async_code()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efbef8a7-b17a-48dc-89b4-0e48a6f9a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14:44"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eaa385a-ddde-4593-882f-3d223a33118e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9f9cc4-8bf1-4902-b8ea-415194ce942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = [os.path.join(directory, f) for f in os.listdir(directory) if f.endswith('.json')]\n",
    "json_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d92934-c3d8-41f3-960d-87b5d0dc0e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(json_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d8b4d0-21e1-4524-ba2d-e286317295fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a99bd1-a62c-48ad-8dbc-156858878aeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e5f1ed-f97d-4718-992a-12602558bfb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0740d3-48a4-4c87-8699-3cc2117904ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
