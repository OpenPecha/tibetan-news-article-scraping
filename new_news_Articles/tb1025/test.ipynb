{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2258a036-edcc-4203-a032-7fd4c75a16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076b685-d470-4ebb-98fe-89e831e0fa06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0893f489-550d-40d7-9ae8-c5a76a07eb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shangri_latibet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54688e52-c01e-4889-946c-c3098c2ab22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_shangri_latibet_page_article_links(url,):\n",
    "    \"\"\"\n",
    "    Extracts all article links from a given shangri_latibet webpage.\n",
    "\n",
    "    This function scrapes the provided URL and extracts links to individual articles\n",
    "    found on the page.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the shangri_latibet webpage containing article links.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"Links\": List[],\n",
    "            \"Message\": string,\n",
    "            \"Response\": int,\n",
    "            \"source_url\": string\n",
    "        }\n",
    "    Raises:\n",
    "    requests.RequestException: If there's an error fetching the webpage.\n",
    "    ValueError: If the expected HTML structure is not found on the page.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    final_response = {\n",
    "        \"Links\": [],\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200,\n",
    "        \"source_url\": url\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=(5, 60-5))\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "\n",
    "        if end_time-start_time > 50:\n",
    "            print(f\"This ULR Took more then 50s: {url}\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # # Getting all the links of articles \n",
    "        all_links = []\n",
    "        all_link_article = soup.find(\"ul\", id=\"breadcrumb\")\n",
    "        if all_link_article:\n",
    "            article_block = all_link_article.find_all(\"a\")\n",
    "            if article_block:\n",
    "                for each_link in article_block:\n",
    "                    article_links = each_link.get(\"href\")\n",
    "                    if article_links:\n",
    "                        all_links.append(article_links)\n",
    "                        \n",
    "        final_response[\"Links\"] = all_links\n",
    "        return final_response\n",
    "     \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"An error occurred while fetching the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except ValueError as e:\n",
    "        # print(f\"An error occurred while parsing the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while parsing the webpage: {e}\"\n",
    "        final_response[\"Response\"] = 404\n",
    "        # getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred: {e}\")\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb97c0-397e-413e-91b6-0d168800449c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c598f0ac-0079-4ccf-a24e-e552628cd629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Links': ['http://www.shangri-latibet.cn/2024-09/19/content_430330.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/19/content_430329.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/18/content_430317.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/12/content_430081.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/12/content_430063.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/12/content_430048.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/12/content_430046.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/11/content_430014.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/11/content_430001.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/11/content_430000.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/10/content_429967.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/06/content_429859.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/05/content_429799.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/04/content_429737.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/04/content_429736.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/04/content_429732.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/03/content_429671.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/03/content_429670.html',\n",
       "  'http://www.shangri-latibet.cn/2024-09/02/content_429606.html',\n",
       "  'http://www.shangri-latibet.cn/2024-08/30/content_429549.html'],\n",
       " 'Message': 'Success',\n",
       " 'Response': 200,\n",
       " 'source_url': 'http://www.shangri-latibet.cn/node_5112.html'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "url = \"http://www.shangri-latibet.cn/node_5112.html\"\n",
    "extract_shangri_latibet_page_article_links(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22674e-af54-45c3-9667-75be1ce42357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0efae0e1-4c58-41bc-8643-fe002b6ca33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_url = 'https://ti.zangdiyg.com/category/index/id/61.html'\n",
    "# print(custom_url.replace(\".html\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7da43a3c-fa6d-4017-aaf8-718649005b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ff984-7034-4f42-aadb-0ced1eff2de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd008b9d-206b-4170-8f45-d6bcb77e2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_shangri_latibet_article_content(url, tags):\n",
    "    headers = {\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'Accept-Encoding': 'gzip, deflate',\n",
    "        'Accept-Language': 'en-US,en;q=0.9,en-IN;q=0.8',\n",
    "        'Cache-Control': 'max-age=0',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Host': 'www.shangri-latibet.cn',\n",
    "        'Referer': 'http://www.shangri-latibet.cn/node_5112_2.html',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/128.0.0.0 Safari/537.36 Edg/128.0.0.0'\n",
    "    }\n",
    "    \n",
    "    # Add If-Modified-Since header\n",
    "    headers['If-Modified-Since'] = (datetime.utcnow() + timedelta(days=18)).strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "    \n",
    "    # Add If-None-Match header\n",
    "    headers['If-None-Match'] = '\"66dc7e9b-58da\"'\n",
    "    \n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': [tags]}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Add a random delay before making the request\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        # Make the request to the URL using the retry session\n",
    "        session = requests_retry_session()\n",
    "        response = session.get(url, headers=headers, allow_redirects=False)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check for redirect\n",
    "        if response.is_redirect:\n",
    "            final_response[\"Message\"] = f\"Redirected to: {response.headers['Location']}\"\n",
    "            final_response[\"Response\"] = response.status_code\n",
    "            return final_response\n",
    "        \n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        tags_body = soup.find(\"ul\", class_=\"breadcrumb\")\n",
    "        tags = []\n",
    "        if tags_body:\n",
    "            tag_list = tags_body.find_all(\"a\")\n",
    "            if tag_list:\n",
    "                for tag in tag_list:\n",
    "                    each_tag = tag.get_text(strip=True)\n",
    "                    tags.append(each_tag)\n",
    "                final_response['data']['meta_data'][\"Tags\"] = tags\n",
    "        \n",
    "        full_body = soup.find('div', class_=\"page\")\n",
    "        if full_body:\n",
    "            # Extract title\n",
    "            title = full_body.find('h1')\n",
    "            if title:\n",
    "                title_text = title.get_text(strip=True)\n",
    "            else:\n",
    "                title_text = \"\"\n",
    "            final_response['data'][\"title\"] = title_text\n",
    "            \n",
    "            metadata = full_body.find('p', class_=\"sx\")\n",
    "            # Extracting Meta Data\n",
    "            try:\n",
    "                if metadata:\n",
    "                    all_text = metadata.get_text(strip=True)\n",
    "                    if all_text:\n",
    "                        text = all_text\n",
    "                        print(text)\n",
    "                        date_pattern = r'དུས་ཚོད།\\s*(\\d{4}-\\d{2}-\\d{2})'\n",
    "                        author_source_pattern = r'(རྩོམ་པ་པོ།|རྩོམ་ཁུངས།)\\s*(.+?)\\s*(?:དུས་ཚོད།|རྩོམ་པ་པོ།|རྩོམ་ཁུངས།|$)'\n",
    "                        # Extract date\n",
    "                        date_match = re.search(date_pattern, text)\n",
    "                        date = date_match.group(1) if date_match else \"\"\n",
    "                        # Extract author and source\n",
    "                        author_source = []\n",
    "                        for match in re.finditer(author_source_pattern, text):\n",
    "                            label = match.group(1)\n",
    "                            content = match.group(2).strip()\n",
    "                            author_source.append(f\"{label} {content}\")\n",
    "                    \n",
    "                        # Combine author and source\n",
    "                        author_source_combined = \" \".join(author_source)\n",
    "                        final_response['data']['meta_data'][\"Date\"] = date\n",
    "                        final_response['data']['meta_data'][\"Author\"] = author_source_combined\n",
    "            except AttributeError:\n",
    "                final_response['data']['meta_data'][\"Author\"] = \"Error fetching author\"\n",
    "                final_response['data']['meta_data'][\"Date\"] = \"Error fetching date\"\n",
    "            \n",
    "        # Extract body content\n",
    "        try:\n",
    "            body = full_body.find(\"div\")\n",
    "            if body:\n",
    "                paragraphs = body.find_all(\"p\")\n",
    "                if paragraphs:\n",
    "                    final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "                else:\n",
    "                    final_response['data']['body'][\"Text\"] = [\"\"]\n",
    "        except AttributeError as e:\n",
    "            final_response['data']['body'][\"Text\"] = [f\"Error fetching body content: {str(e)}\"]\n",
    "        \n",
    "        return final_response\n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c21818ac-8d79-49c1-bf14-2c5866b811ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cdfb3728-4a39-47e2-ab9a-79534a299fcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "རྩོམ་པ་པོ།      ཡིག་བསྒྱུར།   \t\t    དུས་ཚོད།     &nbsp2024-07-01རྩོམ་ཁུངས།      西藏日报藏文媒体\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': {'title': 'གསར་དུ་བཀས་བཅད་པའི་ཐ་སྙད་འདི་དག་ཁྱེད་ཀྱིས་གཟིགས་བྱུང་ངམ།',\n",
       "  'body': {'Audio': '',\n",
       "   'Text': ['半脑山རི་མ་ལྡེབས།',\n",
       "    '脑山རི་ལྡེབས།',\n",
       "    '半舍饲ཕྱེད་ལག་གསོས།\\u2002རྩྭ་ཆས་གཉིས་གསོ།',\n",
       "    '帮虎吃食ངན་པའི་རྒྱབ་ལངས།',\n",
       "    '报喜藏忧བཟང་སྒྲོན་ངན་སྐུང་།',\n",
       "    '倍增效应ལྡབ་འཕར་ནུས་འབྲས།',\n",
       "    '场景式བཟོས་ལྗོངས་རྣམ་པ།',\n",
       "    '车补འཁོར་གསབ（གཞུང་འཁོར་གསབ་དངུལ）',\n",
       "    '车改①རླངས་འཁོར་སྒྱུར་བཟོ། ②གཞུང་འཁོར་སྒྱུར་བཅོས།',\n",
       "    '车载法庭འཁོར་ཐོག་ཁྲིམས་ར།',\n",
       "    '创特提质གསར་གཏོད་སྤུས་འདེགས།',\n",
       "    '错峰游འཚང་གཡོལ་ཡུལ་སྐོར།',\n",
       "    '错峰出门འཚང་གཡོལ་ཕྱིར་སྐྱོད།',\n",
       "    '充电站གློག་གསོག་ས་ཚིགས།',\n",
       "    '大蓬车གུར་འཁོར་ཆེན་པོ།',\n",
       "    '贷签对位བུན་གན་ཐོད་གཏུག',\n",
       "    '电商达人གློག་ཚོང་བརྡར་པ།',\n",
       "    '电信诈骗གློག་འཕྲིན་ཁ་བསླུས།',\n",
       "    '顶雷གཞན་ཉེན་རང་འཁུར།',\n",
       "    '顶流བཞུར་རྩེ（བཞུར་ཚད་རྩེ་གྲས）',\n",
       "    '动力煤སྒུལ་ཤུགས་རྡོ་སོལ།',\n",
       "    '督查问责ལྟ་བཤེར་འགན་འདེད།',\n",
       "    '短视频བརྙན་ཐུང་།',\n",
       "    '对抗性博奕ཁ་གཏད་འཐབ་འགྲན།',\n",
       "    '多功能情节ནུས་མང་བྱུང་ཚུལ།',\n",
       "    '多元共治སྣ་མང་མཉམ་བཅོས།\\u2002རྒྱུ་མང་མཉམ་བཅོས།',\n",
       "    '耳温枪རྣ་མདའ（རྣ་དྲོད་ལྟ་ཆས་ཤིག）',\n",
       "    '仿生材料སྐྱེ་འགྲེའི་རྒྱུ་ཆ（སྐྱེ་དངོས་དཔེ་འགྲེའི་རྒྱུ་ཆ།）',\n",
       "    '非电离性辐射གློག་གྱེས་རང་བཞིན་མིན་པའི་འགྱེད་འཕྲོ།',\n",
       "    '废话文学ཆོ་མེད་རྩོམ་རིག\\u2002དོན་མེད་རྩོམ་རིག',\n",
       "    '废柴①ཤིང་སྙིགས། ②ཟ་འདྲེ།',\n",
       "    '风评གླེང་སྒྲོས།',\n",
       "    '隔代རབས་ཆོད།',\n",
       "    '隔代寄养རབས་ཆོད་བཅོལ་གསོ།\\u2002བུ་གསོ་རྒན་བཅོལ།',\n",
       "    '隔空诊疗བར་ཆོད་སྨན་བཅོས།\\u2002རྒྱང་ཆོད་སྨན་བཅོས།',\n",
       "    '公域流量སྤྱི་ཁོངས་བཞུར་ཚད།',\n",
       "    '共生性腐败མཉམ་རུལ།',\n",
       "    '挂牌督战མིང་སྦྱར་འཐབ་སྐུལ།',\n",
       "    '贯通式མཐུད་སྦྲེལ་རྣམ་པ།',\n",
       "    '光谱实验室འོད་ཤལ་ཚོད་ལྟ་ཁང་།\\u2002འོད་རིམ་ཚོད་ལྟ་ཁང་།',\n",
       "    '海选ཡོངས་འདེམས།',\n",
       "    '航母战斗群གནམ་ཐང་གྲུ་གཟིངས་དམག་རུ།',\n",
       "    '核岛ཉིང་རྡུལ་གླིང་ཕྲན།',\n",
       "    '核心舱ལྟེ་ཚང་།',\n",
       "    '黑暗料理ཟན་ནག་གཡོས་སྦྱོར།',\n",
       "    '花拳绣腿མཛེས་མཛེས་ཤེད་རྩལ།\\u2002ཕྱི་མཛེས་ནང་སྟོང་།',\n",
       "    '灰色产业ཐོན་ལས་སྐྱ་བོ།',\n",
       "    '灰色事态གནས་བབས་སྐྱ་བོ།',\n",
       "    '回波效应ལྡོག་རླབས་ནུས་འབྲས།',\n",
       "    '回溢效应ལྡོག་ལུད་ནུས་འབྲས།',\n",
       "    '机械司法ཁྲིམས་འཛིན་རེངས་པོ།',\n",
       "    '进户线ཁྱིམ་འཁྲིད་སྐུད་ལམ།',\n",
       "    '纪律学习教育སྒྲིག་ཁྲིམས་སློབ་སྦྱོང་སློབ་གསོ།',\n",
       "    '良政善治སྲིད་བཟང་སྐྱོང་མཁས།',\n",
       "    '两山理རི་གཉིས་རིགས་གཞུང་།',\n",
       "    '受警醒、明底线、知敬畏',\n",
       "    'ཉེན་བརྡ་འཕྲོད་པ། མཐའ་ཚད་གསལ་བ། གུས་སྐྲག་ཤེས་པ།',\n",
       "    '“上面千条线，下面一根针”',\n",
       "    '“གོང་ན་སྐུད་སྣེ་སྟོང་།\\u2002གཤམ་ན་ཁབ་མིག་གཅིག”',\n",
       "    '学法、知法、明法、守法',\n",
       "    'ཁྲིམས་སྦྱོང་བ། ཁྲིམས་ཤེས་པ། ཁྲིམས་དམ་པ། ཁྲིམས་སྲུང་བ།',\n",
       "    '学纪、知纪、明纪、守纪',\n",
       "    'སྒྲིག་ཁྲིམས་སྦྱོང་བ། སྒྲིག་ཁྲིམས་ཤེས་པ། \\u2002སྒྲིག་ཁྲིམས་དམ་པ། \\u2002སྒྲིག་ཁྲིམས་སྲུང་བ།',\n",
       "    '政治定力、纪律定力、道德定力、抵腐定力',\n",
       "    'ཆབ་སྲིད་ཀྱི་ངེས་ཤུགས། སྒྲིག་ཁྲིམས་ཀྱི་ངེས་ཤུགས། ཀུན་སྤྱོད་ཀྱི་ངེས་ཤུགས། རུལ་འགོག་གི་ངེས་ཤུགས།',\n",
       "    '知行知止、令行禁止',\n",
       "    'སྤང་བླང་འཛོལ་མེད། \\u2002གང་གསུང་དེ་སྒྲུབ།',\n",
       "    '极简主义སྤྲོས་མེད་རིང་ལུགས།',\n",
       "    '简单主义སྟབས་བདེའི་རིང་ལུགས།',\n",
       "    '急躁症འཚབ་ནད།',\n",
       "    '减贫防贫དབུལ་སྙུང་དབུལ་འགོག',\n",
       "    '建制村སྒྲིག་ཁོངས་གྲོང་ཚོ།',\n",
       "    '靠前服务སྔོན་ཚུད་ཞབས་ཞུ།',\n",
       "    '空飘物མཁའ་ལྡིང་དངོས་རྫས།',\n",
       "    '空壳社ཁོག་སྟོང་མཉམ་ལས་ཁང་།',\n",
       "    '跨境电商ཁོངས་བརྒལ་གློག་ཚོང（ཁོངས་བརྒལ་གློག་རྡུལ་ཚོང་དོན）',\n",
       "    '快评མྱུར་དཔྱད།',\n",
       "    '捆绑消费སྒྲོག་ཚོང་།\\u2002སྡེབ་སྒྲོག་འཛད་སྤྱོད།',\n",
       "    '垃圾短信འཕྲིན་སྙིགས（འཕྲིན་ཐུང་གད་སྙིགས）',\n",
       "    '冷链食品གྲང་ཕྲེང་བཟའ་བཅའ།',\n",
       "    '冷链物流གྲང་ཕྲེང་ཟོག་འགྲུལ།',\n",
       "    '临终关怀师འདའ་ཁའི་ལྟ་སྐྱོང་བ།',\n",
       "    '鲁班工坊ལུའུ་པན་བཟོ་ཁང་།',\n",
       "    '内循环ནང་ཁོངས་འཁོར་རྒྱུག',\n",
       "    '盘绣ཕན་ཤིའུ་ཕར་ཆིན།',\n",
       "    '跑酷འཕག་རྒྱུག',\n",
       "    '批购སྡེབ་ཉོ།',\n",
       "    '平替གོང་ཁེ་ཚབ་ཟོག',\n",
       "    '破冰之旅ལམ་བཤངས་འཚམས་འདྲི།（འབྲེལ་ལམ་བཤངས་པའི་འཚམས་འདྲི）',\n",
       "    '弃访转法ཞུ་བཅར་ཁྲིམས་བསྒྱུར།',\n",
       "    '强制国际རྒྱལ་སྤྱིའི་བཙན་ཁྲིམས།',\n",
       "    '妻贤夫安མོ་མཛངས་ཕོ་སྐྱིད།',\n",
       "    '全封闭管理ཡོངས་སྡོམ་དོ་དམ།',\n",
       "    '亲子游ཕྲུག་ཁྲིད་ཡུལ་སྐོར།',\n",
       "    '拳头整治བཙན་བཅོས（བཙན་འདོམས་བཅོས་སྒྲིག）',\n",
       "    '求生欲གསོན་རེ།',\n",
       "    '热射病ཚད་ནད།',\n",
       "    '人工精神网络མི་ཐབས་དབང་རྩའི་དྲ་རྒྱ།',\n",
       "    '人工修复མི་ཐབས་ཀྱི་ཉམས་གསོ།',\n",
       "    '柔性执法འཇམ་ཐབས་ཁྲིམས་བསྒྱུར།',\n",
       "    '森林康养ནགས་ཁྲོད་ཁམས་གསོ།',\n",
       "    '社区团购སྡེ་ཁུལ་ཚོགས་ཉོ།',\n",
       "    '生态警长སྐྱེ་ཁམས་ཉེན་རྟོག་དཔོན།',\n",
       "    '胜负手རྒྱལ་ཕམ་གཅོད་ཐབས།',\n",
       "    '视频号བརྙན་སྟེགས་ཨང་རྟགས།',\n",
       "    '失温症ལུས་དྲོད་ཉམས་ནད།',\n",
       "    '首发制ཕུད་འགྲེམ་ལམ་ལུགས།',\n",
       "    '数据权利གྲངས་གཞིའི་ཁེ་དབང་།',\n",
       "    '刷脸支付གདོང་བཤེར་དངུལ་སྤྲོད།',\n",
       "    '丝路经济带དར་ལམ་དཔལ་འབྱོར་རྒྱུད།',\n",
       "    '踢脚线རྡོག་ལེན།',\n",
       "    '团体人格ཚོགས་པའི་མི་གཞི།',\n",
       "    '网红地དྲ་གྲགས་ས་ཡུལ།',\n",
       "    '网络靶场དྲ་རྒྱའི་འབེན་ར།',\n",
       "    '网约工དྲ་འབོད་གླ་པ།',\n",
       "    '息诉罢访གཏུག་བཅར་གཉིས་སྤངས།',\n",
       "    '现金流དངུལ་སྨར་རྒྱུག་ཚད།',\n",
       "    '香饽饽①ཤིང་ཏོག་ཤང་པོ་པོ།\\xa0 ②ཨ་ཞིམ་པོ་པོ།\\xa0 ③འཁྱུག་པོ།',\n",
       "    '新风正气སྲོལ་གསར་སྤྱོད་བཟང་།',\n",
       "    '']},\n",
       "  'meta_data': {'URL': 'http://www.shangri-latibet.cn/2024-07/01/content_427343.html',\n",
       "   'Author': 'རྩོམ་པ་པོ། ཡིག་བསྒྱུར། རྩོམ་ཁུངས། 西藏日报藏文媒体',\n",
       "   'Date': '',\n",
       "   'Tags': ['མདུན་ངོས།', 'བོད་ཀྱི་རིག་གནས།', 'དཔེ་གཟིགས།']}},\n",
       " 'Message': 'Success',\n",
       " 'Response': 200}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://www.shangri-latibet.cn/2024-08/29/content_429525.html\"\n",
    "# url = \"https://www.tb1025.cn/lg/qijun/2017-09-29/2318.html\"\n",
    "url = \"http://www.shangri-latibet.cn/2024-03/07/content_422525.html\"\n",
    "url = \"http://www.shangri-latibet.cn/2024-07/01/content_427343.html\"\n",
    "scrape_shangri_latibet_article_content(url, tags=\"དཔྱད་གཏམ།\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe349a6-67ab-49f1-bcba-871bfcb0d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c1dfd-23c4-4748-b2b9-6f32437c4d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf01d677-0fb7-401e-a10f-ce930692af97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: Date not found\n",
      "Author and Source: རྩོམ་པ་པོ། ཡིག་བསྒྱུར། རྩོམ་ཁུངས། 西藏日报藏文媒体\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_info(text):\n",
    "    # Remove extra spaces and replace &nbsp; with a space\n",
    "    text = re.sub(' +', ' ', text)\n",
    "    text = text.replace('&nbsp;', ' ')\n",
    "\n",
    "    # Regular expressions for date and combined author/source\n",
    "    date_pattern = r'དུས་ཚོད།\\s*(\\d{4}-\\d{2}-\\d{2})'\n",
    "    author_source_pattern = r'(རྩོམ་པ་པོ།|རྩོམ་ཁུངས།)\\s*(.+?)\\s*(?:དུས་ཚོད།|རྩོམ་པ་པོ།|རྩོམ་ཁུངས།|$)'\n",
    "\n",
    "    # Extract date\n",
    "    date_match = re.search(date_pattern, text)\n",
    "    if not date_match:\n",
    "        # If date not found, try a more lenient pattern\n",
    "        date_pattern = r'དུས་ཚོད།\\s*(\\d{4}-?\\d{2}-?\\d{2})'\n",
    "        date_match = re.search(date_pattern, text)\n",
    "    date = date_match.group(1) if date_match else \"Date not found\"\n",
    "\n",
    "    # Extract author and source\n",
    "    author_source = []\n",
    "    for match in re.finditer(author_source_pattern, text):\n",
    "        label = match.group(1)\n",
    "        content = match.group(2).strip()\n",
    "        author_source.append(f\"{label} {content}\")\n",
    "\n",
    "    # Combine author and source\n",
    "    author_source_combined = \" \".join(author_source)\n",
    "\n",
    "    return date, author_source_combined\n",
    "\n",
    "# Test with the provided text\n",
    "text = \"རྩོམ་པ་པོ།      ཡིག་བསྒྱུར།   \t\t    དུས་ཚོད།     &nbsp 2024-07-01 རྩོམ་ཁུངས།      西藏日报藏文媒体\"\n",
    "\n",
    "date, author_source = extract_info(text)\n",
    "print(f\"Date: {date}\")\n",
    "print(f\"Author and Source: {author_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dccbe1-d3a1-420c-934b-9b7b460f727b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e469692a-f118-402f-b5c0-f4f9044097a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42227163-9cb1-411f-bf73-6b2605645157",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f5351-4a7c-44b5-b9e6-8f83225c9eb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ec8c0f-dd8f-40f4-918e-75b72b58e9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac70e5d-cf0a-486a-a33a-d1c52b5480d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7662bde0-8cb7-4ac0-bf54-bf48d6d2b047",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "78773b3a-2dec-4d3c-a886-3bbf576411fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date: Date not found\n",
      "Author and Source: རྩོམ་པ་པོ། ཡིག་བསྒྱུར། རྩོམ་ཁུངས། 西藏日报藏文媒体\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_info(text):\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    # Regular expressions for date and combined author/source\n",
    "    date_pattern = r'དུས་ཚོད།\\s*(\\d{4}-\\d{2}-\\d{2})'\n",
    "    author_source_pattern = r'(རྩོམ་པ་པོ།|རྩོམ་ཁུངས།)\\s*(.+?)\\s*(?:དུས་ཚོད།|རྩོམ་པ་པོ།|རྩོམ་ཁུངས།|$)'\n",
    "\n",
    "    # Extract date\n",
    "    date_match = re.search(date_pattern, text)\n",
    "    date = date_match.group(1) if date_match else \"Date not found\"\n",
    "\n",
    "    # Extract author and source\n",
    "    author_source = []\n",
    "    for match in re.finditer(author_source_pattern, text):\n",
    "        label = match.group(1)\n",
    "        content = match.group(2).strip()\n",
    "        author_source.append(f\"{label} {content}\")\n",
    "\n",
    "    # Combine author and source\n",
    "    author_source_combined = \" \".join(author_source)\n",
    "\n",
    "    return date, author_source_combined\n",
    "\n",
    "# Test with the provided text\n",
    "text = \"རྩོམ་པ་པོ།     ཡིག་བསྒྱུར།       དུས་ཚོད།   2024-08-29   རྩོམ་ཁུངས།    སེམས་ཀྱི་ཉི་ཟླ་བོད་ཡིག་དྲ་བ།\"\n",
    "text = \"རྩོམ་པ་པོ།      ཡིག་བསྒྱུར།   \t\t    དུས་ཚོད།     &nbsp 2024-07-01 རྩོམ་ཁུངས།      西藏日报藏文媒体\"\n",
    "\n",
    "date, author_source = extract_info(text)\n",
    "print(f\"Date: {date}\")\n",
    "print(f\"Author and Source: {author_source}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e42c61a-f11e-42c5-96d3-5aeb196b7c39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2694d5f-3417-4794-b20b-cbf44d1c837b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
