{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2258a036-edcc-4203-a032-7fd4c75a16e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "\n",
    "from urllib.parse import urljoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6076b685-d470-4ebb-98fe-89e831e0fa06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c210a518-2417-4c41-bfcf-50b057413de4",
   "metadata": {},
   "source": [
    "kangbatv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54688e52-c01e-4889-946c-c3098c2ab22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_all_kangbatv_page_article_links(url, base_url):\n",
    "    \"\"\"\n",
    "    Extracts all article links from a given kangbatv webpage.\n",
    "\n",
    "    This function scrapes the provided URL and extracts links to individual articles\n",
    "    found on the page.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the kangbatv webpage containing article links.\n",
    "\n",
    "    Returns:\n",
    "        {\n",
    "            \"Links\": List[],\n",
    "            \"Message\": string,\n",
    "            \"Response\": int,\n",
    "            \"source_url\": string\n",
    "        }\n",
    "    Raises:\n",
    "    requests.RequestException: If there's an error fetching the webpage.\n",
    "    ValueError: If the expected HTML structure is not found on the page.\n",
    "    \"\"\"\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    final_response = {\n",
    "        \"Links\": [],\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200,\n",
    "        \"source_url\": url\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=(5, 60-5))\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "\n",
    "        if end_time-start_time > 50:\n",
    "            print(f\"This ULR Took more then 50s: {url}\")\n",
    "        \n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # # Getting all the links of articles \n",
    "        all_links = []\n",
    "        all_link_article = soup.find(\"ul\", class_=\"col-activity-list col-buddhism-list col-article-list\")\n",
    "        if all_link_article:\n",
    "            article_block = all_link_article.find_all(\"li\")\n",
    "            # print(article_block)\n",
    "            if article_block:\n",
    "                for each_head in article_block:\n",
    "                    article_links = each_head.find(\"a\")\n",
    "                    if article_links:\n",
    "                        full_url = urljoin(base_url, article_links.get(\"href\"))\n",
    "                        all_links.append(full_url)\n",
    "        else:\n",
    "            all_link_article = soup.find(\"ul\", class_=\"col-movies-list after\")\n",
    "            if all_link_article:\n",
    "                article_block = all_link_article.find_all(\"li\")\n",
    "                # print(article_block)\n",
    "                if article_block:\n",
    "                    for each_head in article_block:\n",
    "                        article_links = each_head.find(\"a\")\n",
    "                        # print(article_links)\n",
    "                        if article_links:\n",
    "                            full_url = urljoin(base_url, article_links.get(\"href\"))\n",
    "                            all_links.append(full_url)\n",
    "                        \n",
    "        final_response[\"Links\"] = all_links\n",
    "        return final_response\n",
    "     \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        # print(f\"An error occurred while fetching the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except ValueError as e:\n",
    "        # print(f\"An error occurred while parsing the webpage: {e}\")\n",
    "        final_response[\"Message\"] = f\"An error occurred while parsing the webpage: {e}\"\n",
    "        final_response[\"Response\"] = 404\n",
    "        # getattr(e.response, 'status_code', None)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred: {e}\")\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bb97c0-397e-413e-91b6-0d168800449c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed414892-9c45-4db7-a9ae-e56ac4b3775e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c598f0ac-0079-4ccf-a24e-e552628cd629",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Links': ['https://tb.kangbatv.com/xw/gjxw/2024-09-11/694480.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-11/694478.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-11/694477.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-11/694476.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-11/694475.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-10/694377.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-10/694376.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-10/694375.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-10/694374.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-03/693442.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-09-02/693329.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-30/692952.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-28/692581.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-26/692259.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-26/692257.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-26/692256.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-26/692243.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-26/692241.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-21/691595.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-21/691594.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-21/691593.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-20/691390.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-20/691389.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-20/691345.html',\n",
       "  'https://tb.kangbatv.com/xw/gjxw/2024-08-14/690250.html'],\n",
       " 'Message': 'Success',\n",
       " 'Response': 200,\n",
       " 'source_url': 'https://tb.kangbatv.com/xw/gjxw/'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = \"https:\"\n",
    "url = \"https://tb.kangbatv.com/xw/gjxw/\"\n",
    "# url = \"https://tb.kangbatv.com/kbwsxw/\"\n",
    "extract_all_kangbatv_page_article_links(url, base_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22674e-af54-45c3-9667-75be1ce42357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39ff984-7034-4f42-aadb-0ced1eff2de2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd008b9d-206b-4170-8f45-d6bcb77e2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scrape_kangbatv_article_content(url, tags):\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': [tags]}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make the request to the URL\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        tags_body = soup.find(\"ol\", class_=\"cell_6603_ clearfix breadcrumb pull-left\")\n",
    "        tags = []\n",
    "        if tags_body:\n",
    "            tag_list = tags_body.find_all(\"a\")\n",
    "            if tag_list:\n",
    "                for tag in tag_list:\n",
    "                    each_tag = tag.get_text(strip=True)\n",
    "                    tags.append(each_tag)\n",
    "                final_response['data']['meta_data'][\"Tags\"] = tags\n",
    "        \n",
    "        full_body = soup.find('div', class_=\"col-player-box pull-left\")\n",
    "        # print(full_body)\n",
    "        if full_body:\n",
    "            # Extract title\n",
    "            title = soup.find('h3', class_=\"col-text-title text-center article-title\")\n",
    "            if title:\n",
    "                title_text = title.get_text(strip=True)\n",
    "            else:\n",
    "                title_text = \"\"\n",
    "            final_response['data'][\"title\"] = title_text\n",
    "            \n",
    "            metadata = full_body.find('div', class_=\"col-text-brief after\")\n",
    "            # Extracting Meta Data\n",
    "            try:\n",
    "                if len(metadata):\n",
    "                    # Extract date\n",
    "                    date = metadata.find('div', class_=\"col-text-time white-nowrap pull-right\")\n",
    "                    if date:\n",
    "                        final_response['data']['meta_data'][\"Date\"] = date.get_text(strip=True)\n",
    "                        \n",
    "                    # Extract author\n",
    "                    author = metadata.find('div', class_=\"col-text-from white-nowrap pull-right\")\n",
    "                    if author:\n",
    "                        final_response['data']['meta_data'][\"Author\"] = author.get_text(strip=True)\n",
    "                        \n",
    "            except AttributeError:\n",
    "                final_response['data']['meta_data'][\"Author\"] = \"Error fetching author\"\n",
    "                final_response['data']['meta_data'][\"Date\"] = \"Error fetching date\"\n",
    "            \n",
    "            # Extract body content\n",
    "            try:\n",
    "                body = full_body.find(\"div\", class_=\"article-main col-text-cont\")\n",
    "                if body:\n",
    "                    paragraphs = body.find_all(\"p\")\n",
    "                    if paragraphs:\n",
    "                        # Extracting all <p> tags for text content\n",
    "                        final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "                    else:\n",
    "                        final_response['data']['body'][\"Text\"] = [\"\"]\n",
    "    \n",
    "            except AttributeError as e:\n",
    "                final_response['data']['body'][\"Text\"] = [f\"Error fetching body content{str(e)}\"]\n",
    "\n",
    "        else:\n",
    "            full_body = soup.find('div', class_=\"cell_14327_ col-living-player\")\n",
    "            if full_body:\n",
    "                # Extract title\n",
    "                title = soup.find('h1', class_=\"article-title video-title\")\n",
    "                if title:\n",
    "                    title_text = title.get_text(strip=True)\n",
    "                else:\n",
    "                    title_text = \"\"\n",
    "                final_response['data'][\"title\"] = title_text\n",
    "                \n",
    "                metadata_box = full_body.find('table', class_=\"infotab\")\n",
    "                if metadata_box:\n",
    "                    metadata = metadata_box.find_all('td')\n",
    "                    # print(metadata)\n",
    "                    # Extracting Meta Data\n",
    "                    try:\n",
    "                        if len(metadata):\n",
    "                            # Extract date\n",
    "                            date = metadata[1]\n",
    "                            if date:\n",
    "                                final_response['data']['meta_data'][\"Date\"] = date.get_text(strip=True)\n",
    "                                \n",
    "                            # Extract author\n",
    "                            author = metadata[0]\n",
    "                            if author:\n",
    "                                final_response['data']['meta_data'][\"Author\"] = author.get_text(strip=True)\n",
    "                    except AttributeError:\n",
    "                        final_response['data']['meta_data'][\"Author\"] = \"Error fetching author\"\n",
    "                        final_response['data']['meta_data'][\"Date\"] = \"Error fetching date\"\n",
    "        \n",
    "        return final_response\n",
    "        \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        # print(f\"An unexpected error occurred: {e}\")\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca1b1c3-e50c-4d8d-8240-fefd4646a8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4271e0b2-c055-4e50-81e1-dd28da5a468d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00a2489a-7676-4b21-b2b4-17a94f7ca09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def requests_retry_session(\n",
    "    retries=3,\n",
    "    backoff_factor=0.3,\n",
    "    status_forcelist=(500, 502, 504),\n",
    "    session=None,\n",
    "):\n",
    "    session = session or requests.Session()\n",
    "    retry = Retry(\n",
    "        total=retries,\n",
    "        read=retries,\n",
    "        connect=retries,\n",
    "        backoff_factor=backoff_factor,\n",
    "        status_forcelist=status_forcelist,\n",
    "    )\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    return session\n",
    "\n",
    "def scrape_kangbatv_article_content(url, tags):\n",
    "    headers = {\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "            'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "            'Accept-Language': 'en-US,en;q=0.9,en-IN;q=0.8',\n",
    "            'Cache-Control': 'max-age=0',\n",
    "            'Connection': 'keep-alive',\n",
    "            'Host': 'tb.kangbatv.com',\n",
    "            'Referer': 'https://tb.kangbatv.com/xw/gjxw/',\n",
    "            'Sec-Fetch-Dest': 'document',\n",
    "            'Sec-Fetch-Mode': 'navigate',\n",
    "            'Sec-Fetch-Site': 'same-origin',\n",
    "            'Sec-Fetch-User': '?1',\n",
    "            'Upgrade-Insecure-Requests': '1',\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36 Edg/129.0.0.0',\n",
    "            'sec-ch-ua': '\"Microsoft Edge\";v=\"129\", \"Not=A?Brand\";v=\"8\", \"Chromium\";v=\"129\"',\n",
    "            'sec-ch-ua-mobile': '?0',\n",
    "            'sec-ch-ua-platform': '\"Windows\"'\n",
    "        }\n",
    "    \n",
    "    # Add If-Modified-Since header\n",
    "    # headers['If-Modified-Since'] = (datetime.utcnow() + timedelta(days=18)).strftime('%a, %d %b %Y %H:%M:%S GMT')\n",
    "    \n",
    "    # # Add If-None-Match header\n",
    "    # headers['If-None-Match'] = '\"66dc7e9b-58da\"'\n",
    "    \n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': [tags]}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Add a random delay before making the request\n",
    "        time.sleep(random.uniform(1, 3))\n",
    "        \n",
    "        # Make the request to the URL using the retry session\n",
    "        session = requests_retry_session()\n",
    "        response = session.get(url, headers=headers, allow_redirects=False)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        # Check for redirect\n",
    "        if response.is_redirect:\n",
    "            final_response[\"Message\"] = f\"Redirected to: {response.headers['Location']}\"\n",
    "            final_response[\"Response\"] = response.status_code\n",
    "            return final_response\n",
    "        \n",
    "        # Parse the page content with BeautifulSoup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "\n",
    "        tags_body = soup.find(\"ol\", class_=\"cell_6603_ clearfix breadcrumb pull-left\")\n",
    "        tags = []\n",
    "        if tags_body:\n",
    "            tag_list = tags_body.find_all(\"a\")\n",
    "            if tag_list:\n",
    "                for tag in tag_list:\n",
    "                    each_tag = tag.get_text(strip=True)\n",
    "                    tags.append(each_tag)\n",
    "                final_response['data']['meta_data'][\"Tags\"] = tags\n",
    "        \n",
    "        full_body = soup.find('div', class_=\"col-player-box pull-left\")\n",
    "        # print(full_body)\n",
    "        if full_body:\n",
    "            # Extract title\n",
    "            title = soup.find('h3', class_=\"col-text-title text-center article-title\")\n",
    "            if title:\n",
    "                title_text = title.get_text(strip=True)\n",
    "            else:\n",
    "                title_text = \"\"\n",
    "            final_response['data'][\"title\"] = title_text\n",
    "            \n",
    "            metadata = full_body.find('div', class_=\"col-text-brief after\")\n",
    "            # Extracting Meta Data\n",
    "            try:\n",
    "                if len(metadata):\n",
    "                    # Extract date\n",
    "                    date = metadata.find('div', class_=\"col-text-time white-nowrap pull-right\")\n",
    "                    if date:\n",
    "                        final_response['data']['meta_data'][\"Date\"] = date.get_text(strip=True)\n",
    "                        \n",
    "                    # Extract author\n",
    "                    author = metadata.find('div', class_=\"col-text-from white-nowrap pull-right\")\n",
    "                    if author:\n",
    "                        final_response['data']['meta_data'][\"Author\"] = author.get_text(strip=True)\n",
    "                        \n",
    "            except AttributeError:\n",
    "                final_response['data']['meta_data'][\"Author\"] = \"Error fetching author\"\n",
    "                final_response['data']['meta_data'][\"Date\"] = \"Error fetching date\"\n",
    "            \n",
    "            # Extract body content\n",
    "            try:\n",
    "                body = full_body.find(\"div\", class_=\"article-main col-text-cont\")\n",
    "                if body:\n",
    "                    paragraphs = body.find_all(\"p\")\n",
    "                    if paragraphs:\n",
    "                        # Extracting all <p> tags for text content\n",
    "                        final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "                    else:\n",
    "                        final_response['data']['body'][\"Text\"] = [\"\"]\n",
    "    \n",
    "            except AttributeError as e:\n",
    "                final_response['data']['body'][\"Text\"] = [f\"Error fetching body content{str(e)}\"]\n",
    "\n",
    "        else:\n",
    "            full_body = soup.find('div', class_=\"cell_14327_ col-living-player\")\n",
    "            if full_body:\n",
    "                # Extract title\n",
    "                title = soup.find('h1', class_=\"article-title video-title\")\n",
    "                if title:\n",
    "                    title_text = title.get_text(strip=True)\n",
    "                else:\n",
    "                    title_text = \"\"\n",
    "                final_response['data'][\"title\"] = title_text\n",
    "                \n",
    "                metadata_box = full_body.find('table', class_=\"infotab\")\n",
    "                if metadata_box:\n",
    "                    metadata = metadata_box.find_all('td')\n",
    "                    # print(metadata)\n",
    "                    # Extracting Meta Data\n",
    "                    try:\n",
    "                        if len(metadata):\n",
    "                            # Extract date\n",
    "                            date = metadata[1]\n",
    "                            if date:\n",
    "                                final_response['data']['meta_data'][\"Date\"] = date.get_text(strip=True)\n",
    "                                \n",
    "                            # Extract author\n",
    "                            author = metadata[0]\n",
    "                            if author:\n",
    "                                final_response['data']['meta_data'][\"Author\"] = author.get_text(strip=True)\n",
    "                    except AttributeError:\n",
    "                        final_response['data']['meta_data'][\"Author\"] = \"Error fetching author\"\n",
    "                        final_response['data']['meta_data'][\"Date\"] = \"Error fetching date\"\n",
    "        \n",
    "        return final_response\n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408  # Request Timeout\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "cdfb3728-4a39-47e2-ab9a-79534a299fcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'title': 'པ་ཞིས་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པར་རྟེན་འབྲེལ་མཛད་སྒོ་བསྡུས་པ།',\n",
       "  'body': {'Audio': '',\n",
       "   'Text': ['',\n",
       "    'པར་འདི་ནི་ཟླ9པའི་ཚེས7ཉིན་པ་ཞི་རྒྱལ་ཁབ་ཀྱི་མཁའ་དམག་གིས་པ་ཞི་ལི་ཡ་རུ་བསྡུས་པའི་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོའི་སྟེང་གནམ་གྲུ་འཁྲབ་སྟོན་བྱེད་བཞིན་པའི་ཚུལ་རེད།',\n",
       "    'ཞིན་ཧྭ་གསར་འགྱུར་ཁང་གི་གློག་འཕྲིན་ལྟར།ཟླ9པའི་ཚེས7ཉིན།\\u3000པ་ཞི་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོ་པ་ཞི་ལི་ཡ་རུ་བསྡུས།\\u3000པ་ཞིའི་ཙུང་ཐུང་ལུའུ་ལ་ཡིས་ཉིན་དེའི་དམག་གཟིགས་མཛད་སྒོར་ཞུགས་ཏེ།\\u3000པ་ཞིའི་མཁའ་དམག་དང་མཚོ་དམག\\u3000སྐམ་དམག\\u3000ཁྲབ་ལྡན་དམག་དཔུང་།\\u3000དམག་སྲིད་ཉེན་རྟོག་དམག་དཔུང་།\\u3000མེ་གསོད་དམག་དཔུང།\\u3000འཕུར་སྐྱོད་འཁྲབ་སྟོན་རུ་ཁག་བཅས་ལ་གཟིགས་ཞིབ་གནང་བ་མ་ཟད།\\u3000སྤྱི་ཚོགས་ལས་རིགས་སོ་སོས་རྩ་འཛུགས་བྱས་པའི་ཁྲོམ་སྐོར་བྱ་འགུལ་ལ་བལྟས།\\u3000།',\n",
       "    '',\n",
       "    'པར་འདི་ནི་ཟླ9པའི་ཚེས7ཉིན་པ་ཞི་རྒྱལ་ཁབ་ཀྱི་རྒྱལ་ས་པ་ཞི་ལི་ཡ་རུ་པ་ཞིའི་ཙུང་ཐུང་ལུའུ་ལ་ཡིས་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོར་ཞུགས་པའི་ཚུལ་རེད།',\n",
       "    '',\n",
       "    'པར་འདི་ནི་ཟླ9པའི་ཚེས7ཉིན་ལྟད་མོ་བས་པ་ཞི་རྒྱལ་ཁབ་ཀྱི་རྒྱལ་ས་པ་ཞི་ལི་ཡ་རུ་བསྡུས་པའི་པ་ཞི་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོ་སྟེང་གནམ་གྲུའི་འཁྲབ་སྟོན་ལ་ལྟ་བཞིན་པའི་ཚུལ་རེད།',\n",
       "    '',\n",
       "    'པར་འདི་ནི་ཟླ9པའི་ཚེས7ཉིན་པ་ཞི་རྒྱལ་ཁབ་ཀྱི་རྒྱལ་ས་པ་ཞི་ལི་ཡ་རུ་པ་ཞིའི་ཙུང་ཐུང་ལུའུ་ལ་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོར་ཞུགས་པའི་ཚུལ་རེད།',\n",
       "    '',\n",
       "    'པར་འདི་ནི་ཟླ9པའི་ཚེས7ཉིན་པ་ཞི་རྒྱལ་ཁབ་ཀྱི་རྒྱལ་ས་པ་ཞི་ལི་ཡ་རུ་དམག་མིས་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོར་ཞུགས་པའི་ཚུལ་རེད།',\n",
       "    '',\n",
       "    'པར་འདི་ནི་ཟླ9པའི་ཚེས7ཉིན་པ་ཞི་རྒྱལ་ཁབ་ཀྱི་རྒྱལ་ས་པ་ཞི་ལི་ཡ་རུ་དམག་མིས་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོར་ཞུགས་པའི་ཚུལ་རེད།',\n",
       "    '',\n",
       "    'པར་འདི་ནི་ཟླ9པའི་ཚེས7ཉིན་པ་ཞི་རྒྱལ་ཁབ་ཀྱི་རྒྱལ་ས་པ་ཞི་ལི་ཡ་རུ་དམག་མིའི་རོལ་ཆ་རུ་ཁག་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོར་ཞུགས་པའི་ཚུལ་རེད།',\n",
       "    '',\n",
       "    'པར་འདི་ནི་ཟླ9པའི་ཚེས7ཉིན་པ་ཞི་རྒྱལ་ཁབ་ཀྱི་རྒྱལ་ས་པ་ཞི་ལི་ཡ་རུ་དམངས་ཚོགས་རྣམས་རྒྱལ་ཁབ་སྒེར་ཚུགས་ཐོབ་ནས་ལོ་འཁོར202ལོན་པའི་རྟེན་འབྲེལ་མཛད་སྒོར་ཞུགས་པའི་ཚུལ་རེད།',\n",
       "    '']},\n",
       "  'meta_data': {'URL': 'https://tb.kangbatv.com/xw/gjxw/2024-09-11/694480.html',\n",
       "   'Author': 'ཞིན་ཧྭ་དྲ་བ།',\n",
       "   'Date': '2024-09-11',\n",
       "   'Tags': ['མདུན་ངོས།', 'གསར་འགྱུར།', 'རྒྱལ་སྤྱིའི་གསར་འགྱུར།']}},\n",
       " 'Message': 'Success',\n",
       " 'Response': 200}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# url = \"https://tb.kangbatv.com/kbwsxw/2020-09-25/345119.html\"\n",
    "url = \"https://tb.kangbatv.com/xw/gjxw/2024-09-11/694480.html\"\n",
    "scrape_kangbatv_article_content(url, tags=\"དཔྱད་གཏམ།\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe349a6-67ab-49f1-bcba-871bfcb0d8ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28c1dfd-23c4-4748-b2b9-6f32437c4d8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf01d677-0fb7-401e-a10f-ce930692af97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
