{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any, List\n",
    "import time\n",
    "\n",
    "def extract_all_article_links(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Extracts all article links from a given RFA (Radio Free Asia) webpage.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the RFA webpage containing article links.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing article links and status details.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    final_response = {\n",
    "        \"Links\": [],\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.get(url, headers=headers, timeout=(5, 60-5))\n",
    "        response.raise_for_status()\n",
    "        end_time = time.time()\n",
    "        if end_time - start_time > 50:\n",
    "            print(f\"This URL took more than 50s: {url}\")\n",
    "\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        all_articles = soup.find_all(\"div\", class_=\"teaserimg\")\n",
    "        if not all_articles:\n",
    "            raise ValueError(\"Could not find the main article container on the page.\")\n",
    "        \n",
    "        article_links = []\n",
    "        for article in all_articles:\n",
    "            links = article.find_all(\"a\", class_=\"teaserimg\")\n",
    "            article_links.extend([link.get(\"href\") for link in links if link.get(\"href\")])\n",
    "        \n",
    "        final_response[\"Links\"] = article_links\n",
    "        return final_response\n",
    "    \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408\n",
    "        return final_response\n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "    except ValueError as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while parsing the webpage: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "    except Exception as e:\n",
    "        final_response[\"Message\"] = f\"An unexpected error occurred: {e}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        return final_response\n",
    "\n",
    "\n",
    "def scrape_rfa_article(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrapes an article from the RFA (Radio Free Asia) website.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the RFA article to scrape.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing the scraped information and status details.\n",
    "    \"\"\"\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    \n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': []}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url, headers=headers, timeout=120)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.find('h2', class_='no_media')\n",
    "        final_response['data']['title'] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "        # Extracting Meta Data\n",
    "        meta_data_body = soup.find('div', class_='sectionteaser archive')\n",
    "        if meta_data_body:\n",
    "            author_name = meta_data_body.find('a', class_='author')\n",
    "            final_response['data']['meta_data'][\"Author\"] = author_name.get_text() if author_name else \"Author not found\"\n",
    "            \n",
    "            date_time = meta_data_body.find('span', class_='story_date')\n",
    "            final_response['data']['meta_data'][\"Date\"] = date_time.get_text() if date_time else \"Date not found\"\n",
    "        \n",
    "        # Getting tag meta data \n",
    "        tag_meta = soup.find('ul', class_='tags')\n",
    "        if tag_meta:\n",
    "            tag_meta = tag_meta.select('li a')\n",
    "            final_response['data']['meta_data'][\"Tags\"] = [tag.text for tag in tag_meta]\n",
    "        \n",
    "        # Extract body content\n",
    "        body = soup.find('div', class_='storytext')\n",
    "        if body:\n",
    "            paragraphs = body.find_all('p')\n",
    "            final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "\n",
    "            # Find the audio tag and get its src attribute\n",
    "            audio = body.find('audio')\n",
    "            if audio:\n",
    "                final_response['data']['body'][\"Audio\"] = audio.get('src', \"No audio source found\")\n",
    "        \n",
    "        return final_response\n",
    "    \n",
    "    except requests.Timeout:\n",
    "        final_response[\"Message\"] = \"Request timed out\"\n",
    "        final_response[\"Response\"] = 408\n",
    "        return final_response\n",
    "        \n",
    "    except requests.RequestException as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {e}\"\n",
    "        final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "        return final_response\n",
    "\n",
    "\n",
    "def fetch_all_articles(base_url: str, total_pages: int) -> List[str]:\n",
    "    \"\"\"\n",
    "    Fetches article links from all pages of the RFA website.\n",
    "\n",
    "    Args:\n",
    "    base_url (str): The base URL of the RFA archive.\n",
    "    total_pages (int): The total number of pages to scrape.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: A list of article URLs.\n",
    "    \"\"\"\n",
    "    all_article_links = []\n",
    "    for page in range(total_pages):\n",
    "        url = f\"{base_url}&start={page * 13}\"\n",
    "        response = extract_all_article_links(url)\n",
    "        if response[\"Response\"] == 200:\n",
    "            all_article_links.extend(response[\"Links\"])\n",
    "        else:\n",
    "            print(f\"Failed to fetch links from page {page + 1}: {response['Message']}\")\n",
    "    return all_article_links\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage\n",
    "base_archive_url = \"https://www.rfa.org/tibetan/story_archive?uids=e2b0f97935844f15a765322387ba2381%40c80eb7880fab441bb93e888d5fb0b8d7%40a34e3f118cfb47c89efc8d10ca29beb4%40ed8b21d453ee4bdbb56632bdf00ba533%404f49bd08f0c44e9384542e0d5f9c37f8%402f794fee5a644fdb9964a3accdff9162%403defd73dd1994e0db35cb48f998d6958%401b63853142714c3489c3e1c3fa848623%406153403d00e24f4a8b178048cc9a1820%4069e4cbeeacf1491c95f326788021710e%401fe2a601fca941b7a3f781e0cd25954b%40bc0393320a3445688ec79b1187a883bf%4042fa45d86fca4d11befaecf0a942246d%406d988d19310843b18445bfb0f461ba8f%404a2437b9cd884233ad46554a0303ef89%407a0ed25eec54437f976e8f99fed6ee84%4009849b68da104e188c5ae11d7112827b\"\n",
    "total_pages = 3888\n",
    "\n",
    "# Fetch all article links\n",
    "all_article_links = fetch_all_articles(base_archive_url, total_pages)\n",
    "\n",
    "# Fetch all article links\n",
    "all_article_links = fetch_all_articles(base_archive_url, start_index, num_pages)\n",
    "\n",
    "# Scrape each article\n",
    "for article_url in all_article_links:\n",
    "    article_data = scrape_rfa_article(article_url)\n",
    "    print(article_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': {'title': '',\n",
       "  'body': {'Audio': '', 'Text': []},\n",
       "  'meta_data': {'URL': 'https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html',\n",
       "   'Author': '',\n",
       "   'Date': '',\n",
       "   'Tags': []}},\n",
       " 'Message': 'An error occurred while fetching the article: 403 Client Error: Forbidden for url: https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html',\n",
       " 'Response': 403}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_rfa_article(\"https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "from fake_useragent import UserAgent\n",
    "\n",
    "def scrape_rfa_article(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrapes an article from the RFA (Radio Free Asia) website with multiple attempts to bypass 403 errors.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the RFA article to scrape.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing the scraped information and status details.\n",
    "    \"\"\"\n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': []}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "\n",
    "    def parse_content(content):\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        \n",
    "        # Extract title\n",
    "        title = soup.find('h2', class_='no_media')\n",
    "        final_response['data']['title'] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "        # Extracting Meta Data\n",
    "        meta_data_body = soup.find('div', class_='sectionteaser archive')\n",
    "        if meta_data_body:\n",
    "            author_name = meta_data_body.find('a', class_='author')\n",
    "            final_response['data']['meta_data'][\"Author\"] = author_name.get_text() if author_name else \"Author not found\"\n",
    "            \n",
    "            date_time = meta_data_body.find('span', class_='story_date')\n",
    "            final_response['data']['meta_data'][\"Date\"] = date_time.get_text() if date_time else \"Date not found\"\n",
    "        \n",
    "        # Getting tag meta data \n",
    "        tag_meta = soup.find('ul', class_='tags')\n",
    "        if tag_meta:\n",
    "            tag_meta = tag_meta.select('li a')\n",
    "            final_response['data']['meta_data'][\"Tags\"] = [tag.text for tag in tag_meta]\n",
    "        \n",
    "        # Extract body content\n",
    "        body = soup.find('div', class_='storytext')\n",
    "        if body:\n",
    "            paragraphs = body.find_all('p')\n",
    "            final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "\n",
    "            # Find the audio tag and get its src attribute\n",
    "            audio = body.find('audio')\n",
    "            if audio:\n",
    "                final_response['data']['body'][\"Audio\"] = audio.get('src', \"No audio source found\")\n",
    "\n",
    "    def attempt_request(headers, delay=0):\n",
    "        try:\n",
    "            if delay > 0:\n",
    "                time.sleep(delay)\n",
    "            response = requests.get(url, headers=headers, timeout=120)\n",
    "            response.raise_for_status()\n",
    "            parse_content(response.content)\n",
    "            return True\n",
    "        except requests.RequestException as e:\n",
    "            final_response[\"Message\"] = f\"An error occurred while fetching the article: {e}\"\n",
    "            final_response[\"Response\"] = getattr(e.response, 'status_code', 500)\n",
    "            print(f\"failed in attempt_request: {e}\")\n",
    "            return False\n",
    "\n",
    "    # Attempt 1: Original headers\n",
    "    original_headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "    }\n",
    "    print(\"testing Attempt 1: Original headers\")\n",
    "    if attempt_request(original_headers):\n",
    "        return final_response\n",
    "\n",
    "    # Attempt 2: Random User-Agent\n",
    "    print(\"testing Attempt 2: Random User-Agent\")\n",
    "    ua = UserAgent()\n",
    "    random_headers = {'User-Agent': ua.random}\n",
    "    if attempt_request(random_headers):\n",
    "        return final_response\n",
    "\n",
    "    # Attempt 3: Full headers set\n",
    "    print(\"testing Attempt 3: Full headers set\")\n",
    "    full_headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "        'Accept-Language': 'en-US,en;q=0.5',\n",
    "        'Referer': 'https://www.google.com/',\n",
    "        'DNT': '1',\n",
    "        'Connection': 'keep-alive',\n",
    "        'Upgrade-Insecure-Requests': '1',\n",
    "    }\n",
    "    if attempt_request(full_headers):\n",
    "        return final_response\n",
    "\n",
    "    # Attempt 4: With delay\n",
    "    print(\"testing Attempt 4:  With delay\")\n",
    "    if attempt_request(full_headers, delay=2):\n",
    "        return final_response\n",
    "\n",
    "    return final_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing Attempt 1: Original headers\n",
      "failed in attempt_request: 403 Client Error: Forbidden for url: https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\n",
      "testing Attempt 2: Random User-Agent\n",
      "failed in attempt_request: 403 Client Error: Forbidden for url: https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\n",
      "testing Attempt 3: Full headers set\n",
      "failed in attempt_request: 403 Client Error: Forbidden for url: https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\n",
      "testing Attempt 4:  With delay\n",
      "failed in attempt_request: 403 Client Error: Forbidden for url: https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'data': {'title': '',\n",
       "  'body': {'Audio': '', 'Text': []},\n",
       "  'meta_data': {'URL': 'https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html',\n",
       "   'Author': '',\n",
       "   'Date': '',\n",
       "   'Tags': []}},\n",
       " 'Message': 'An error occurred while fetching the article: 403 Client Error: Forbidden for url: https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html',\n",
       " 'Response': 403}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scrape_rfa_article(\"https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data': {'title': '', 'body': {'Audio': '', 'Text': []}, 'meta_data': {'URL': 'https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html', 'Author': '', 'Date': '', 'Tags': []}}, 'Message': 'An error occurred while fetching the article: Message: \\nStacktrace:\\n\\tGetHandleVerifier [0x00007FF65C2A9642+30946]\\n\\t(No symbol) [0x00007FF65C25E3D9]\\n\\t(No symbol) [0x00007FF65C156FDA]\\n\\t(No symbol) [0x00007FF65C1A822C]\\n\\t(No symbol) [0x00007FF65C1A850C]\\n\\t(No symbol) [0x00007FF65C1EDCB7]\\n\\t(No symbol) [0x00007FF65C1CCAAF]\\n\\t(No symbol) [0x00007FF65C1EB041]\\n\\t(No symbol) [0x00007FF65C1CC813]\\n\\t(No symbol) [0x00007FF65C19A6E5]\\n\\t(No symbol) [0x00007FF65C19B021]\\n\\tGetHandleVerifier [0x00007FF65C3DF84D+1301229]\\n\\tGetHandleVerifier [0x00007FF65C3EBDC7+1351783]\\n\\tGetHandleVerifier [0x00007FF65C3E2A13+1313971]\\n\\tGetHandleVerifier [0x00007FF65C2DDD16+245686]\\n\\t(No symbol) [0x00007FF65C26759F]\\n\\t(No symbol) [0x00007FF65C263814]\\n\\t(No symbol) [0x00007FF65C2639A2]\\n\\t(No symbol) [0x00007FF65C25A3FF]\\n\\tBaseThreadInitThunk [0x00007FFB7C90257D+29]\\n\\tRtlUserThreadStart [0x00007FFB7D74AF28+40]\\n', 'Response': 500}\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "def scrape_rfa_article_selenium(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrapes an article from the RFA (Radio Free Asia) website using Selenium.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the RFA article to scrape.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing the scraped information and status details.\n",
    "    \"\"\"\n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': []}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "\n",
    "    options = Options()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    try:\n",
    "        driver = webdriver.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "\n",
    "        # Wait for the content to load\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"storytext\"))\n",
    "        )\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find('h2', class_='no_media')\n",
    "        final_response['data']['title'] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "        # Extracting Meta Data\n",
    "        meta_data_body = soup.find('div', class_='sectionteaser archive')\n",
    "        if meta_data_body:\n",
    "            author_name = meta_data_body.find('a', class_='author')\n",
    "            final_response['data']['meta_data'][\"Author\"] = author_name.get_text() if author_name else \"Author not found\"\n",
    "            \n",
    "            date_time = meta_data_body.find('span', class_='story_date')\n",
    "            final_response['data']['meta_data'][\"Date\"] = date_time.get_text() if date_time else \"Date not found\"\n",
    "        \n",
    "        # Getting tag meta data \n",
    "        tag_meta = soup.find('ul', class_='tags')\n",
    "        if tag_meta:\n",
    "            tag_meta = tag_meta.select('li a')\n",
    "            final_response['data']['meta_data'][\"Tags\"] = [tag.text for tag in tag_meta]\n",
    "        \n",
    "        # Extract body content\n",
    "        body = soup.find('div', class_='storytext')\n",
    "        if body:\n",
    "            paragraphs = body.find_all('p')\n",
    "            final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "\n",
    "            # Find the audio tag and get its src attribute\n",
    "            audio = body.find('audio')\n",
    "            if audio:\n",
    "                final_response['data']['body'][\"Audio\"] = audio.get('src', \"No audio source found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return final_response\n",
    "\n",
    "# Usage\n",
    "url = \"https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\"\n",
    "result = scrape_rfa_article_selenium(url)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done Chrome driver\n",
      "{'data': {'title': '', 'body': {'Audio': '', 'Text': []}, 'meta_data': {'URL': 'https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html', 'Author': '', 'Date': '', 'Tags': []}}, 'Message': 'An error occurred while fetching the article: Message: \\nStacktrace:\\n\\tGetHandleVerifier [0x004F8923+23283]\\n\\t(No symbol) [0x004BE934]\\n\\t(No symbol) [0x003F0733]\\n\\t(No symbol) [0x0043326F]\\n\\t(No symbol) [0x004334AB]\\n\\t(No symbol) [0x0046EE42]\\n\\t(No symbol) [0x00454464]\\n\\t(No symbol) [0x0046CB8D]\\n\\t(No symbol) [0x004541B6]\\n\\t(No symbol) [0x00428017]\\n\\t(No symbol) [0x0042890D]\\n\\tGetHandleVerifier [0x005EA5F3+1013699]\\n\\tGetHandleVerifier [0x005F3E4C+1052700]\\n\\tGetHandleVerifier [0x005ED4B4+1025668]\\n\\tGetHandleVerifier [0x0051EA2B+179195]\\n\\t(No symbol) [0x004C6833]\\n\\t(No symbol) [0x004C3198]\\n\\t(No symbol) [0x004C3337]\\n\\t(No symbol) [0x004BB4BE]\\n\\tBaseThreadInitThunk [0x76197BA9+25]\\n\\tRtlInitializeExceptionChain [0x7791C10B+107]\\n\\tRtlClearBits [0x7791C08F+191]\\n', 'Response': 500}\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "def scrape_rfa_article_undetected(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrapes an article from the RFA (Radio Free Asia) website using undetected-chromedriver.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the RFA article to scrape.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing the scraped information and status details.\n",
    "    \"\"\"\n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': []}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "    try:\n",
    "        driver = uc.Chrome(options=options)\n",
    "        driver.get(url)\n",
    "        print(\"done Chrome driver\")\n",
    "\n",
    "        # Wait for the content to load\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CLASS_NAME, \"storytext\"))\n",
    "        )\n",
    "        print(\"done loading driver\")\n",
    "\n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        print(soup)\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find('h2', class_='no_media')\n",
    "        final_response['data']['title'] = title.text.strip() if title else \"Title not found\"\n",
    "\n",
    "        # Extracting Meta Data\n",
    "        meta_data_body = soup.find('div', class_='sectionteaser archive')\n",
    "        if meta_data_body:\n",
    "            author_name = meta_data_body.find('a', class_='author')\n",
    "            final_response['data']['meta_data'][\"Author\"] = author_name.get_text() if author_name else \"Author not found\"\n",
    "            \n",
    "            date_time = meta_data_body.find('span', class_='story_date')\n",
    "            final_response['data']['meta_data'][\"Date\"] = date_time.get_text() if date_time else \"Date not found\"\n",
    "        \n",
    "        # Getting tag meta data \n",
    "        tag_meta = soup.find('ul', class_='tags')\n",
    "        if tag_meta:\n",
    "            tag_meta = tag_meta.select('li a')\n",
    "            final_response['data']['meta_data'][\"Tags\"] = [tag.text for tag in tag_meta]\n",
    "        \n",
    "        # Extract body content\n",
    "        body = soup.find('div', class_='storytext')\n",
    "        if body:\n",
    "            paragraphs = body.find_all('p')\n",
    "            final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "\n",
    "            # Find the audio tag and get its src attribute\n",
    "            audio = body.find('audio')\n",
    "            if audio:\n",
    "                final_response['data']['body'][\"Audio\"] = audio.get('src', \"No audio source found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "    finally:\n",
    "        driver.quit()\n",
    "\n",
    "    return final_response\n",
    "\n",
    "# Usage\n",
    "url = \"https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\"\n",
    "result = scrape_rfa_article_undetected(url)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chrome driver initialized\n",
      "Navigated to URL: https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\n",
      "Timeout waiting for storytext element. Proceeding with available content.\n",
      "Page source length: 375\n",
      "<html><head>\n",
      "<title>Access Denied</title>\n",
      "</head><body>\n",
      "<h1>Access Denied</h1>\n",
      " \n",
      "You don't have permission to access \"http://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\" on this server.<p>\n",
      "Reference #18.556c3f17.1724129745.32982be8\n",
      "</p><p>https://errors.edgesuite.net/18.556c3f17.1724129745.32982be8</p>\n",
      "</body></html>\n",
      "Title: Title not found\n",
      "{'data': {'title': 'Title not found', 'body': {'Audio': '', 'Text': []}, 'meta_data': {'URL': 'https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html', 'Author': '', 'Date': '', 'Tags': []}}, 'Message': 'Success', 'Response': 200}\n"
     ]
    }
   ],
   "source": [
    "import undetected_chromedriver as uc\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, Any\n",
    "import time\n",
    "\n",
    "def scrape_rfa_article_undetected(url: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Scrapes an article from the RFA (Radio Free Asia) website using undetected-chromedriver.\n",
    "\n",
    "    Args:\n",
    "    url (str): The URL of the RFA article to scrape.\n",
    "\n",
    "    Returns:\n",
    "    Dict[str, Any]: A dictionary containing the scraped information and status details.\n",
    "    \"\"\"\n",
    "    final_response = {\n",
    "        \"data\": {\n",
    "            'title': \"\",\n",
    "            'body': {\"Audio\": \"\", \"Text\": []},\n",
    "            'meta_data': {'URL': url, 'Author': \"\", 'Date': \"\", 'Tags': []}\n",
    "        },\n",
    "        \"Message\": \"Success\",\n",
    "        \"Response\": 200\n",
    "    }\n",
    "\n",
    "    options = uc.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    options.add_argument('--no-sandbox')\n",
    "    options.add_argument('--disable-dev-shm-usage')\n",
    "    options.add_argument('--disable-blink-features=AutomationControlled')\n",
    "    options.add_argument('--disable-extensions')\n",
    "    options.add_argument('--disable-gpu')\n",
    "    options.add_argument('--window-size=1920x1080')\n",
    "\n",
    "    try:\n",
    "        driver = uc.Chrome(options=options)\n",
    "        print(\"Chrome driver initialized\")\n",
    "        \n",
    "        driver.get(url)\n",
    "        print(f\"Navigated to URL: {url}\")\n",
    "        \n",
    "        time.sleep(10)  # Add a static wait to allow for any initial loading\n",
    "        \n",
    "        try:\n",
    "            # Wait for the content to load\n",
    "            WebDriverWait(driver, 30).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"storytext\"))\n",
    "            )\n",
    "            print(\"storytext element found\")\n",
    "        except TimeoutException:\n",
    "            print(\"Timeout waiting for storytext element. Proceeding with available content.\")\n",
    "        \n",
    "        # Get the page source even if the specific element wasn't found\n",
    "        page_source = driver.page_source\n",
    "        print(f\"Page source length: {len(page_source)}\")\n",
    "        \n",
    "        # Parse the page source with BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "        print(soup)\n",
    "\n",
    "        # Extract title\n",
    "        title = soup.find('h2', class_='no_media')\n",
    "        final_response['data']['title'] = title.text.strip() if title else \"Title not found\"\n",
    "        print(f\"Title: {final_response['data']['title']}\")\n",
    "\n",
    "        # Extracting Meta Data\n",
    "        meta_data_body = soup.find('div', class_='sectionteaser archive')\n",
    "        if meta_data_body:\n",
    "            author_name = meta_data_body.find('a', class_='author')\n",
    "            final_response['data']['meta_data'][\"Author\"] = author_name.get_text() if author_name else \"Author not found\"\n",
    "            \n",
    "            date_time = meta_data_body.find('span', class_='story_date')\n",
    "            final_response['data']['meta_data'][\"Date\"] = date_time.get_text() if date_time else \"Date not found\"\n",
    "        \n",
    "        # Getting tag meta data \n",
    "        tag_meta = soup.find('ul', class_='tags')\n",
    "        if tag_meta:\n",
    "            tag_meta = tag_meta.select('li a')\n",
    "            final_response['data']['meta_data'][\"Tags\"] = [tag.text for tag in tag_meta]\n",
    "        \n",
    "        # Extract body content\n",
    "        body = soup.find('div', class_='storytext')\n",
    "        if body:\n",
    "            paragraphs = body.find_all('p')\n",
    "            final_response['data']['body'][\"Text\"] = [para.get_text(strip=True) for para in paragraphs]\n",
    "            print(f\"Number of paragraphs found: {len(final_response['data']['body']['Text'])}\")\n",
    "\n",
    "            # Find the audio tag and get its src attribute\n",
    "            audio = body.find('audio')\n",
    "            if audio:\n",
    "                final_response['data']['body'][\"Audio\"] = audio.get('src', \"No audio source found\")\n",
    "\n",
    "    except Exception as e:\n",
    "        final_response[\"Message\"] = f\"An error occurred while fetching the article: {str(e)}\"\n",
    "        final_response[\"Response\"] = 500\n",
    "        print(f\"Error: {str(e)}\")\n",
    "    finally:\n",
    "        if 'driver' in locals():\n",
    "            driver.quit()\n",
    "\n",
    "    return final_response\n",
    "\n",
    "# Usage\n",
    "url = \"https://www.rfa.org/tibetan/sargyur/richard-r-verma-will-lead-u-s-delegations-to-nepal-08192024143818.html\"\n",
    "result = scrape_rfa_article_undetected(url)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
